{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUwTv1Wf30hi",
        "outputId": "a2e81e50-aa6f-4f0a-e8b0-a753118a0a18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[all] in /usr/local/lib/python3.7/dist-packages (0.25.2)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[all]) (4.13.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[all]) (1.5.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[all]) (0.0.8)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[all]) (1.21.6)\n",
            "Requirement already satisfied: pytest==7.0.1 in /usr/local/lib/python3.7/dist-packages (from gym[all]) (7.0.1)\n",
            "Requirement already satisfied: box2d-py==2.3.5 in /usr/local/lib/python3.7/dist-packages (from gym[all]) (2.3.5)\n",
            "Requirement already satisfied: imageio>=2.14.1 in /usr/local/lib/python3.7/dist-packages (from gym[all]) (2.22.3)\n",
            "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.7/dist-packages (from gym[all]) (3.2.2)\n",
            "Requirement already satisfied: lz4>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from gym[all]) (4.0.2)\n",
            "Requirement already satisfied: pygame==2.1.0 in /usr/local/lib/python3.7/dist-packages (from gym[all]) (2.1.0)\n",
            "Requirement already satisfied: mujoco==2.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[all]) (2.2.0)\n",
            "Requirement already satisfied: ale-py~=0.7.5 in /usr/local/lib/python3.7/dist-packages (from gym[all]) (0.7.5)\n",
            "Requirement already satisfied: opencv-python>=3.0 in /usr/local/lib/python3.7/dist-packages (from gym[all]) (4.6.0.66)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.7/dist-packages (from gym[all]) (4.1.0)\n",
            "Requirement already satisfied: mujoco-py<2.2,>=2.1 in /usr/local/lib/python3.7/dist-packages (from gym[all]) (2.1.2.14)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from mujoco==2.2.0->gym[all]) (1.3.0)\n",
            "Requirement already satisfied: glfw in /usr/local/lib/python3.7/dist-packages (from mujoco==2.2.0->gym[all]) (2.5.5)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.7/dist-packages (from mujoco==2.2.0->gym[all]) (3.1.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytest==7.0.1->gym[all]) (21.3)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.7/dist-packages (from pytest==7.0.1->gym[all]) (1.0.0)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest==7.0.1->gym[all]) (2.0.1)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.7/dist-packages (from pytest==7.0.1->gym[all]) (1.1.1)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from pytest==7.0.1->gym[all]) (22.1.0)\n",
            "Requirement already satisfied: py>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from pytest==7.0.1->gym[all]) (1.11.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[all]) (5.10.0)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.7/dist-packages (from imageio>=2.14.1->gym[all]) (9.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[all]) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[all]) (4.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->gym[all]) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->gym[all]) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->gym[all]) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->gym[all]) (0.11.0)\n",
            "Requirement already satisfied: cffi>=1.10 in /usr/local/lib/python3.7/dist-packages (from mujoco-py<2.2,>=2.1->gym[all]) (1.15.1)\n",
            "Requirement already satisfied: fasteners~=0.15 in /usr/local/lib/python3.7/dist-packages (from mujoco-py<2.2,>=2.1->gym[all]) (0.18)\n",
            "Requirement already satisfied: Cython>=0.27.2 in /usr/local/lib/python3.7/dist-packages (from mujoco-py<2.2,>=2.1->gym[all]) (0.29.32)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.10->mujoco-py<2.2,>=2.1->gym[all]) (2.21)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.0->gym[all]) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license] in /usr/local/lib/python3.7/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license]) (1.21.6)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license]) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license]) (4.13.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license]) (1.5.0)\n",
            "Requirement already satisfied: autorom[accept-rom-license]~=0.4.2 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license]) (0.4.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (2.23.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (5.10.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (7.1.2)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (0.4.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license]) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license]) (3.10.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (2022.9.24)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[classic_control] in /usr/local/lib/python3.7/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[classic_control]) (1.21.6)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[classic_control]) (0.0.8)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[classic_control]) (1.5.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[classic_control]) (4.13.0)\n",
            "Requirement already satisfied: pygame==2.1.0 in /usr/local/lib/python3.7/dist-packages (from gym[classic_control]) (2.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[classic_control]) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[classic_control]) (4.1.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install gym[all]\n",
        "!pip install gym[accept-rom-license]\n",
        "!pip install gym[classic_control]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "LTXaC76Y1zLv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2qA2PPmcJmH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
        "import traceback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vGcXyI4qeOh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "from collections import deque\n",
        "from gym import spaces\n",
        "import cv2\n",
        "cv2.ocl.setUseOpenCL(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tC-OffQM3yr2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhbqJzltq48b"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining the Replay Buffer Class"
      ],
      "metadata": {
        "id": "O-tM2chcxuu2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_5zoLauq6XH"
      },
      "outputs": [],
      "source": [
        "\n",
        "# create replay buffer of tuples of (state, next_state, action, reward, done)\n",
        "# Initially keep storing the samples in the Buffers, then start sampling from it for getting IID Samples\n",
        "class ReplayBuffer():\n",
        "    def __init__(self, max_size=1e6):\n",
        "        \n",
        "        self.max_size = max_size # Max size of Replay Buffer\n",
        "        self.storage = []\n",
        "        self.pointer = 0\n",
        "\n",
        "    def add(self, data):\n",
        "        if len(self.storage) == self.max_size:\n",
        "            self.storage[int(self.pointer)] = data\n",
        "            self.pointer = (self.pointer + 1) % self.max_size\n",
        "        else:\n",
        "            self.storage.append(data)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        uniform_samples = np.random.randint(0, len(self.storage), size=batch_size) # pi\n",
        "        x, y, u, r, d = [], [], [], [], []\n",
        "\n",
        "        for i in uniform_samples : \n",
        "            X, Y, U, R, D = self.storage[i] # Using Optimal Control terminology--> \n",
        "            u.append(np.array(U, copy=False))# Control Uk ( the optimal control term for Action )\n",
        "            r.append(np.array(R, copy=False)) # reward \n",
        "            d.append(np.array(D, copy=False)) # Buffer size of Replay Buffer\n",
        "            x.append(np.array(X, copy=False)) # Xk - current state\n",
        "            y.append(np.array(Y, copy=False))\n",
        "            \n",
        "\n",
        "        return np.array(x), np.array(y), np.array(u).reshape(-1,1), np.array(r).reshape(-1,1), np.array(d).reshape(-1,1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sblfFwxmrGKx"
      },
      "source": [
        "# Environment Wrappers Copied from Open AI Baseline BELOW"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ** ** *** ** ** *** *** *** ** *** *** *** *** ** ** ** *** *** *** *** *** *** *** *** ** *** *** ** ** ***"
      ],
      "metadata": {
        "id": "Y5-rapnk3LMf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lBuNwYsq-bc"
      },
      "outputs": [],
      "source": [
        "# from: https://github.com/openai/baselines/baselines/common/atari_wrappers.py\n",
        "# from: https://github.com/Officium/RL-Experiments/blob/master/src/common/wrappers.py \n",
        "\n",
        "class NoopResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env, noop_max=30):\n",
        "        \"\"\"Sample initial states by taking random number of no-ops on reset.\n",
        "        No-op is assumed to be action 0.\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.noop_max = noop_max\n",
        "        self.override_num_noops = None\n",
        "        self.noop_action = 0\n",
        "        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n",
        "        self.env.reset(**kwargs)\n",
        "        if self.override_num_noops is not None:\n",
        "            noops = self.override_num_noops\n",
        "        else:\n",
        "            noops = self.unwrapped.np_random.randint(1, self.noop_max + 1)\n",
        "        assert noops > 0\n",
        "        obs = None\n",
        "        for _ in range(noops):\n",
        "            obs, _, done, _ = self.env.step(self.noop_action)\n",
        "            if done:\n",
        "                obs = self.env.reset(**kwargs)\n",
        "        return obs\n",
        "\n",
        "    def step(self, ac):\n",
        "        return self.env.step(ac)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8KbUgdw-rFe4"
      },
      "outputs": [],
      "source": [
        "class FireResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        \"\"\"Take action on reset for environments that are fixed until firing.\"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
        "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        self.env.reset(**kwargs)\n",
        "        obs, _, done, _ = self.env.step(1)\n",
        "        if done:\n",
        "            self.env.reset(**kwargs)\n",
        "        obs, _, done, _ = self.env.step(2)\n",
        "        if done:\n",
        "            self.env.reset(**kwargs)\n",
        "        return obs\n",
        "\n",
        "    def step(self, ac):\n",
        "        return self.env.step(ac)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f58GhVfnrdfR"
      },
      "outputs": [],
      "source": [
        "class EpisodicLifeEnv(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n",
        "        Done by DeepMind for the DQN and co. since it helps value estimation.\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.lives = 0\n",
        "        self.was_real_done = True\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, done, info = self.env.step(action)\n",
        "        self.was_real_done = done\n",
        "        # check current lives, make loss of life terminal,\n",
        "        # then update lives to handle bonus lives\n",
        "        lives = self.env.unwrapped.ale.lives()\n",
        "        if 0 < lives < self.lives:\n",
        "            # for Qbert sometimes we stay in lives == 0 condition for a few\n",
        "            # frames so it's important to keep lives > 0, so that we only reset\n",
        "            # once the environment advertises done.\n",
        "            done = True\n",
        "        self.lives = lives\n",
        "        return obs, reward, done, info\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        \"\"\"Reset only when lives are exhausted.\n",
        "        This way all states are still reachable even though lives are episodic,\n",
        "        and the learner need not know about any of this behind-the-scenes.\n",
        "        \"\"\"\n",
        "        if self.was_real_done:\n",
        "            obs = self.env.reset(**kwargs)\n",
        "        else:\n",
        "            # no-op step to advance from terminal/lost life state\n",
        "            obs, _, _, _ = self.env.step(0)\n",
        "        self.lives = self.env.unwrapped.ale.lives()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlmgc3nwrdiA"
      },
      "outputs": [],
      "source": [
        "class MaxAndSkipEnv(gym.Wrapper):\n",
        "    def __init__(self, env, skip=4):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        # most recent raw observations (for max pooling across time steps)\n",
        "        shape = (2, ) + env.observation_space.shape\n",
        "        self._obs_buffer = np.zeros(shape, dtype=np.uint8)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Repeat action, sum reward, and max over last observations.\"\"\"\n",
        "        total_reward = 0.0\n",
        "        done = info = None\n",
        "        for i in range(self._skip):\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            if i == self._skip - 2:\n",
        "                self._obs_buffer[0] = obs\n",
        "            if i == self._skip - 1:\n",
        "                self._obs_buffer[1] = obs\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        # Note that the observation on the done=True frame doesn't matter\n",
        "        max_frame = self._obs_buffer.max(axis=0)\n",
        "\n",
        "        return max_frame, total_reward, done, info\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        return self.env.reset(**kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFVZ02lTrdkm"
      },
      "outputs": [],
      "source": [
        "class ClipRewardEnv(gym.RewardWrapper):\n",
        "    def __init__(self, env):\n",
        "        gym.RewardWrapper.__init__(self, env)\n",
        "\n",
        "    @staticmethod\n",
        "    def reward(reward):\n",
        "        \"\"\"Bin reward to {+1, 0, -1} by its sign.\"\"\"\n",
        "        return np.sign(reward)\n",
        "\n",
        "\n",
        "class WarpFrame(gym.ObservationWrapper):\n",
        "    def __init__(self, env, width=84, height=84, grayscale=True):\n",
        "        \"\"\"Warp frames to 84x84 as done in the Nature paper and later work.\"\"\"\n",
        "        gym.ObservationWrapper.__init__(self, env)\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "        self.grayscale = grayscale\n",
        "        shape = (1 if self.grayscale else 3, self.height, self.width)\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=0, high=255, shape=shape, dtype=np.uint8\n",
        "        )\n",
        "\n",
        "    def observation(self, frame):\n",
        "        if self.grayscale:\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "        size = (self.width, self.height)\n",
        "        frame = cv2.resize(frame, size, interpolation=cv2.INTER_AREA)\n",
        "        if self.grayscale:\n",
        "            frame = np.expand_dims(frame, -1)\n",
        "        return frame.transpose((2, 0, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ty_N-FpYrpso"
      },
      "outputs": [],
      "source": [
        "class FrameStack(gym.Wrapper):\n",
        "    def __init__(self, env, k):\n",
        "        \"\"\"Stack k last frames.\n",
        "        Returns lazy array, which is much more memory efficient.\n",
        "        See Also `LazyFrames`\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.k = k\n",
        "        self.frames = deque([], maxlen=k)\n",
        "        shp = env.observation_space.shape\n",
        "        shape = (shp[0] * k, ) + shp[1:]\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=0, high=255, shape=shape, dtype=env.observation_space.dtype\n",
        "        )\n",
        "\n",
        "    def reset(self):\n",
        "        ob = self.env.reset()\n",
        "        for _ in range(self.k):\n",
        "            self.frames.append(ob)\n",
        "        return np.asarray(self._get_ob())\n",
        "\n",
        "    def step(self, action):\n",
        "        ob, reward, done, info = self.env.step(action)\n",
        "        self.frames.append(ob)\n",
        "        return np.asarray(self._get_ob()), reward, done, info\n",
        "\n",
        "    def _get_ob(self):\n",
        "        assert len(self.frames) == self.k\n",
        "        return LazyFrames(list(self.frames))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYbVBTVjrdn8"
      },
      "outputs": [],
      "source": [
        "class LazyFrames(object):\n",
        "    def __init__(self, frames):\n",
        "        \"\"\"This object ensures that common frames between the observations are\n",
        "        only stored once. It exists purely to optimize memory usage which can be\n",
        "        huge for DQN's 1M frames replay buffers.\n",
        "        This object should only be converted to numpy array before being passed\n",
        "        to the model. You'd not believe how complex the previous solution was.\n",
        "        \"\"\"\n",
        "        self._frames = frames\n",
        "        self._out = None\n",
        "\n",
        "    def _force(self):\n",
        "        if self._out is None:\n",
        "            self._out = np.concatenate(self._frames, axis=-3)\n",
        "            self._frames = None\n",
        "        return self._out\n",
        "\n",
        "    def __array__(self, dtype=None):\n",
        "        out = self._force()\n",
        "        if dtype is not None:\n",
        "            out = out.astype(dtype)\n",
        "        return out\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._force())\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self._force()[i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CEXmS0CNrFhX"
      },
      "outputs": [],
      "source": [
        "class ScaledFloatFrame(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        gym.ObservationWrapper.__init__(self, env)\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=env.observation_space.shape, dtype=np.float32)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        # careful! This undoes the memory optimization, use\n",
        "        # with smaller replay buffers only.\n",
        "        return np.array(observation).astype(np.float32) / 255.0\n",
        "    \n",
        "    \n",
        "def wrap_deepmind(env, episode_life=True, clip_rewards=True, frame_stack=False, scale=False):\n",
        "    \"\"\"Configure environment for DeepMind-style Atari.\n",
        "    \"\"\"\n",
        "    if episode_life:\n",
        "        env = EpisodicLifeEnv(env)\n",
        "    if 'FIRE' in env.unwrapped.get_action_meanings():\n",
        "        env = FireResetEnv(env)\n",
        "    env = WarpFrame(env, width=84, height=84)\n",
        "    env = MaxAndSkipEnv(env, skip=4)\n",
        "    if scale:\n",
        "        env = ScaledFloatFrame(env)\n",
        "    if clip_rewards:\n",
        "        env = ClipRewardEnv(env)\n",
        "    if frame_stack:\n",
        "        env = FrameStack(env, 4)\n",
        "    return env"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ** ** *** ** ** *** *** *** ** *** *** *** *** ** ** ** *** *** *** *** *** *** *** *** ** *** *** ** ** ***"
      ],
      "metadata": {
        "id": "r1styb7d3eVu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAdDCS1DDDjj"
      },
      "source": [
        "# Wrapper Env code ends here. NOTE - The ABOVE Wrapper code is from OpenAI baseline "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q1.a Loading the Pong Environment & Printing the details of Next State & rewards by interacting with the Environment"
      ],
      "metadata": {
        "id": "pfRUNZocAn8m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBoYn8pPrFkE",
        "outputId": "3b36f302-c669-4168-8dae-3fe2c203039c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:594: UserWarning: \u001b[33mWARN: The environment Pong-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
            "  f\"The environment {id} is out of date. You should consider \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:318: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:44: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:298: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
            "  \"No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  \"Core environment is written in old step API which returns one bool instead of two. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action Space--> Discrete(6)\n",
            "Observation Space--> Box(0, 255, (210, 160, 3), uint8)\n",
            "reward range--> (-inf, inf)\n",
            "Meta data -->  {'render_modes': ['human', 'rgb_array']}\n",
            "Specifications --> EnvSpec(id='Pong-v0', entry_point='gym.envs.atari:AtariEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=10000, order_enforce=True, autoreset=False, disable_env_checker=False, new_step_api=False, kwargs={'game': 'pong', 'obs_type': 'rgb', 'repeat_action_probability': 0.25, 'full_action_space': False, 'frameskip': (2, 5)}, namespace=None, name='Pong', version=0)\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is -1.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is -1.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is -1.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is -1.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            " reward is 0.0\n",
            "Episode finished after 1440 timesteps with reward 0.0\n",
            "max reward is --> 0.0 and min reward --> -1.0\n"
          ]
        }
      ],
      "source": [
        "#create Pong env and test it a bit\n",
        "#env = gym.make('PongNoFrameskip-v4') #\n",
        "env = gym.make(\"Pong-v0\")#gym.make(\"ALE/Pong-v5\") #gym.make(\"Pong-v0\")#('PongNoFrameskip-v4')\n",
        "print('Action Space-->',env.action_space)\n",
        "print('Observation Space-->',env.observation_space)\n",
        "print('reward range-->',env.reward_range)\n",
        "print('Meta data --> ',env.metadata)\n",
        "print('Specifications -->',env.spec)\n",
        "env.reset()\n",
        "all_rewards =[]\n",
        "#for i in range(3000):\n",
        "timestamp =0\n",
        "sample_count = 250\n",
        "while True :\n",
        "    timestamp += 1\n",
        "    env.render(mode = 'rgb_array')\n",
        "    action = env.action_space.sample()\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    if sample_count >0:\n",
        "      print(' reward is {}'.format(reward ))\n",
        "      all_rewards.append(reward)\n",
        "      sample_count -= 1\n",
        "\n",
        "    if done:\n",
        "        print('Episode finished after {} timesteps with reward {}'.format(timestamp, reward))\n",
        "        \n",
        "        env.reset()\n",
        "        break\n",
        "print('max reward is --> {} and min reward --> {}'.format(max(all_rewards), min(all_rewards)))\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpwyA1AsELq_"
      },
      "source": [
        "# Deep Q Network - Here a CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWpCsR3aEQQ6"
      },
      "outputs": [],
      "source": [
        "# create Deep Q Network Class by inheriting from torch.nn.Module\n",
        "            \n",
        "class DeepQNet(nn.Module):\n",
        "    def __init__(self, action_space_size, hidden_size):\n",
        "        # defining the convolution & dense layers\n",
        "        super(DeepQNet, self).__init__()\n",
        "        self.convolution_layer_1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n",
        "        self.convolution_layer_2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
        "        self.convolution_layer_3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
        "        self.dense_layer = nn.Linear(7 * 7 * 64, hidden_size)\n",
        "        self.out_layer = nn.Linear(hidden_size, action_space_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x / 255. # image data is stored as ints in 0 to 255 range. Divide to scale to 0 to 1 range\n",
        "        x = F.relu(self.convolution_layer_1(x))\n",
        "        x = F.relu(self.convolution_layer_2(x))\n",
        "        x = F.relu(self.convolution_layer_3(x))\n",
        "        x = F.relu(self.dense_layer(x.view(x.size(0), -1)))\n",
        "        return self.out_layer(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ky4xTGkLEuDi"
      },
      "source": [
        "# DQN Agent Classn- methods for training & epsilon greedy action selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmzCYK_fEvrb"
      },
      "outputs": [],
      "source": [
        "class DQNAgent():\n",
        "    def __init__(self, action_space_size, hidden_size, learning_rate ):\n",
        "        self.action_space_size = action_space_size\n",
        "        self.target_net = DeepQNet(action_space_size, hidden_size).to(device)# moving the model to GPU memory\n",
        "        self.train_net = DeepQNet(action_space_size, hidden_size).to(device)\n",
        "        \n",
        "        self.target_net.load_state_dict(self.train_net.state_dict()) # copying train net parameters to target net\n",
        "        self.optimizer = optim.Adam(self.train_net.parameters(), lr=learning_rate)\n",
        "\n",
        "        \n",
        "    def select_action(self, states, epsilon):\n",
        "        # applying Epsilon-Greedy method for selecting the Next Action\n",
        "        if np.random.rand() <= epsilon :\n",
        "            action = env.action_space.sample() # with Epsilon probability, choose actions randomly\n",
        "        else:\n",
        "            # GREEDY action is the LARGEST Q value from the train network based on the input. with Probability (1-Epsilon) we do Greedy Action selection\n",
        "            with torch.no_grad():\n",
        "                input_state = torch.FloatTensor(np.array(states)).unsqueeze(0).to(device)\n",
        "                action = self.train_net(input_state).max(1)[1]#.view(1, 1)#.detach().cpu().numpy()[0]\n",
        "                action = int(action)\n",
        "        return action\n",
        "\n",
        "    \n",
        "    def train(self, replay_buffer, batch_size, discount_factor):\n",
        "        # train the training network\n",
        "        # sample a batch from the replay buffer\n",
        "        x0, x1, action, reward, done = replay_buffer.sample(batch_size)\n",
        "        # convert these batches of Samples into tensors and attack to GPU if available\n",
        "        state_batch = torch.FloatTensor(x0).to(device)\n",
        "        next_state_batch = torch.FloatTensor(x1).to(device)\n",
        "        action_batch = torch.LongTensor(action).to(device)\n",
        "        reward_batch = torch.FloatTensor(reward).to(device)\n",
        "        done_batch = torch.FloatTensor(1. - done).to(device)\n",
        "\n",
        "        # get train net Q values\n",
        "        train_q = self.train_net(state_batch).gather(1, action_batch)\n",
        "        \n",
        "        # get target net Q values\n",
        "        with torch.no_grad():\n",
        "            target_net_q = reward_batch + done_batch * discount_factor * \\\n",
        "                     torch.max( self.target_net(next_state_batch).detach(), dim=1)[0].view(batch_size, -1)\n",
        "            \n",
        "        # get loss between train q values and target q values\n",
        "            # DQN implementations typically use MSE loss or Huber loss (smooth_l1_loss is similar to Huber)\n",
        "        #loss_fn = nn.MSELoss()\n",
        "        #loss = loss_fn(train_q, target_net_q) \n",
        "        loss = F.smooth_l1_loss(train_q, target_net_q)\n",
        "        \n",
        "        # optimize the parameters with the loss\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        for param in self.train_net.parameters():\n",
        "            param.grad.data.clamp_(-1, 1)\n",
        "        self.optimizer.step()\n",
        "        # we return the loss so we can monitor loss and debug the network if necessary\n",
        "        return loss.detach().cpu().numpy()   \n",
        "    \n",
        "    \n",
        "    def update_target_network(self, num_iter, update_every):\n",
        "        # update target network \"update_every\" iterations, SET in the Next HYPERPARAMETER cell\n",
        "        # hard target network update: updates target network fully with train network params\n",
        "        if num_iter % update_every == 0:\n",
        "            #print(\"Updating target network parameters\")\n",
        "            self.target_net.load_state_dict(self.train_net.state_dict())    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ii_XFrqoG0BE"
      },
      "source": [
        "# SET Hyperparameters + Create Deep Q Net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwOZfDl_Ejuq"
      },
      "outputs": [],
      "source": [
        "# Initialize env and set up hyperparameters\n",
        "env = gym.make(\"Pong-v0\") #env = gym.make('PongNoFrameskip-v4')\n",
        "\n",
        "# wrap env\n",
        "env = wrap_deepmind(env, episode_life=True, clip_rewards=True, frame_stack=True, scale=False)\n",
        "action_space_size = env.action_space.n\n",
        "\n",
        "# set seed\n",
        "seed = 31\n",
        "env.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# hyperparameters\n",
        "timesteps = 2000000 # run env for this many time steps--- We are supposed to run a 2 or 2.5 million minimum\n",
        "#timesteps =  20000 # run env for this many time steps--- We are supposed to run a 2 or 2.5 million minimum\n",
        "hidden_size = 512   # side of hidden layer of FFNN that connects CNN to outputs\n",
        "\n",
        "discount_factor = 0.99 # discount future states by\n",
        "learning_rate = 0.0001 # learning rate of optimizer\n",
        "batch_size = 32    # size of batch trained on\n",
        "start_training_after = 10001 # start training NN after this many timesteps\n",
        "\n",
        "epsilon_start = 1.0 # epsilon greedy start value\n",
        "epsilon_min = 0.01  # epsilon greedy end value\n",
        "epsilon_decay_steps = timesteps * .15 # decay epsilon over this many timesteps\n",
        "epsilon_step = (epsilon_start - epsilon_min)/(epsilon_decay_steps) # decrement epsilon by this amount every timestep\n",
        "\n",
        "update_target_every = 1000 # update target network every this steps\n",
        "\n",
        "# create replay buffer\n",
        "replay_size = 50000 # size of replay buffer\n",
        "replay_buffer = ReplayBuffer(max_size=replay_size)\n",
        "\n",
        "# create DQN Agent\n",
        "dqn_agent = DQNAgent(action_space_size, hidden_size, learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Colab Mount & PATH creation & Checkpoint Save & Retrieval"
      ],
      "metadata": {
        "id": "YEKAE9rU4Uy6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os.path\n",
        "from os import path\n",
        "RL_A4_path = '/content/drive/MyDrive/Colab_Notebooks_IITH/RL_A4'\n",
        "if path.exists(RL_A4_path) == False:\n",
        "  os.makedirs(RL_A4_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKQndS6138hr",
        "outputId": "b9b11ab3-0098-40ef-cb63-79de8ef117cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import sys \n",
        "\n",
        "def save_checkpoint(epochs, train_net, target_net, optimizer, reward=0, latest_episode =0,  stats_every=10):\n",
        "    \"\"\"\n",
        "    Function to save the trained model to disk.\n",
        "    \"\"\"\n",
        "    print(f\"Saving checkpoint...\")\n",
        "    torch.save({\n",
        "                'epoch': epochs,\n",
        "                'train_net_state_dict': train_net.state_dict(),\n",
        "                'target_net_state_dict': train_net.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                }, os.path.join(RL_A4_path, 'DQN_Pong.pth'))\n",
        "    with open(os.path.join(RL_A4_path, str(stats_every)+\"episode_mean_reward.txt\"),\"a\") as myfile: # a- Append mode\n",
        "              myfile.write(str(latest_episode)+'_'+str(epochs)+'_'+str(reward)+'\\n')\n",
        "    \n",
        "\n",
        "def load_checkpoint(RL_A4_path , train_net,target_net , optimizer):\n",
        "    checkpoint = torch.load(os.path.join(RL_A4_path, 'DQN_Pong.pth'))\n",
        "    train_net.load_state_dict(checkpoint['train_net_state_dict'])\n",
        "    target_net.load_state_dict(checkpoint['target_net_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    return train_net,target_net, optimizer, checkpoint['epoch']\n",
        "\n",
        "def load_Reward_Seq(checkpoint_fpath):\n",
        "  my_file = open(os.path.join(RL_A4_path, str(stats_every)+\"episode_mean_reward.txt\"), \"r\")\n",
        "  data = my_file.read()\n",
        "  my_file.close()\n",
        "  #data_into_list = data.split(\"\\n\")[:-1]\n",
        "  #data_into_list = list(map(int, data_into_list))\n",
        "  data_into_list = data.split(\"\\n\")\n",
        "  episode_num = [int(x.split('_')[0]) for x in data_into_list if '_' in x  ]\n",
        "  epochs = [int(x.split('_')[1])  for x in data_into_list if '_' in x  ]\n",
        "  mean_reward = [float(x.split('_')[2])  for x in data_into_list if '_' in x  ]\n",
        "  return episode_num, epochs, mean_reward"
      ],
      "metadata": {
        "id": "hyGGfDLI6Rdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEhNtMeiHAu4"
      },
      "source": [
        "# Training Parameters Initializations"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stats_rewards_list = [] # store traininig statistics for plotting in this\n",
        "stats_every = 10 # print the traininig statistics every this many episodes\n",
        "\n",
        "epsilon = epsilon_start\n",
        "state = env.reset()\n",
        "load_from_checkpoint = 1\n",
        "\n",
        "total_reward = 0\n",
        "episode = 1\n",
        "episode_length = 0\n",
        "stats_loss = 0."
      ],
      "metadata": {
        "id": "EZMdOtAiknby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Starts Next"
      ],
      "metadata": {
        "id": "x80BWpie1C6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "if load_from_checkpoint ==1 :\n",
        "  try :\n",
        "    dqn_agent.train_net,dqn_agent.target_net, dqn_agent.optimizer, epochs  = load_checkpoint(RL_A4_path, \n",
        "                                                                                             dqn_agent.train_net,\n",
        "                                                                                             dqn_agent.target_net , \n",
        "                                                                                             dqn_agent.optimizer)\n",
        "    episodes,_,_= load_Reward_Seq(RL_A4_path)\n",
        "    episode = episodes[-1]\n",
        "    #(checkpoint_fpath , train_net,target_net , optimizer)\n",
        "    print('Restarted training from Checkpoint. Previous checkpoint --> {} epochs. Episode--> {}'.format(epochs, episode))\n",
        "    episode+= 1\n",
        "  except Exception as e: \n",
        "    epochs = 0 \n",
        "    print(e)\n",
        "    \n",
        "\n",
        "for ts in range(epochs, timesteps+epochs):\n",
        "    #env.render()\n",
        "    # select an action from the agent's policy\n",
        "    action = dqn_agent.select_action(state, epsilon)\n",
        "    # decay epsilon\n",
        "    epsilon -= epsilon_step\n",
        "    if epsilon < epsilon_min:\n",
        "        epsilon = epsilon_min\n",
        "            \n",
        "    # enter action into the env\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    total_reward += reward\n",
        "    episode_length += 1\n",
        "    \n",
        "    # add experience to replay buffer\n",
        "    replay_buffer.add((state, next_state, action, reward, float(done)))\n",
        "    \n",
        "    if ts > start_training_after:\n",
        "        # train the agent\n",
        "        stats_loss += dqn_agent.train(replay_buffer, batch_size, discount_factor)\n",
        "        # update the target network every (if conditions are met in update_target_network)\n",
        "        dqn_agent.update_target_network(ts, update_target_every)\n",
        "    \n",
        "    if done:\n",
        "        state = env.reset()\n",
        "        stats_rewards_list.append((episode, total_reward, episode_length))\n",
        "        episode += 1\n",
        "        total_reward = 0\n",
        "        episode_length = 0\n",
        "\n",
        "        if ts > start_training_after and episode % stats_every == 0:\n",
        "            n_episode_mean_reward =  np.mean(stats_rewards_list[-stats_every:],axis=0)[1]\n",
        "            #save_checkpoint(epochs, train_net, target_net, optimizer, reward=0, latest_episode =0,  stats_every=10)\n",
        "            save_checkpoint(ts, dqn_agent.train_net,dqn_agent.target_net,  dqn_agent.optimizer, n_episode_mean_reward, episode, stats_every)\n",
        "            print('Episode: {};'.format(episode),' Timestep: {};'.format(ts),stats_every,\n",
        "                '-Episode Mean reward: {:.1f};'.format(n_episode_mean_reward ),\n",
        "                'Episode length: {:.1f};'.format(np.mean(stats_rewards_list[-stats_every:],axis=0)[2]),\n",
        "                'Epsilon: {:.2f};'.format(epsilon), 'Loss: {:.4f}'.format(stats_loss))\n",
        "            stats_loss = 0.\n",
        "        \n",
        "        # stopping condition for training if agent reaches the amount of reward\n",
        "        if len(stats_rewards_list) > stats_every and np.mean(stats_rewards_list[-stats_every:],axis=0)[1] > 19:\n",
        "            print(\"Stopping at episode {} with average rewards of {} in last {} episodes\".\n",
        "                format(episode, np.mean(stats_rewards_list[-stats_every:],axis=0)[1], stats_every))\n",
        "            break  \n",
        "    else:\n",
        "        state = next_state"
      ],
      "metadata": {
        "id": "kFnKGY513sXD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8729af20-7b70-45f3-825b-ce958c903e00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/content/drive/MyDrive/Colab_Notebooks_IITH/RL_A4/DQN_Pong.pth'\n",
            "Saving checkpoint...\n",
            "Episode: 40;  Timestep: 11383; 10 -Episode Mean reward: -20.8; Episode length: 298.2; Epsilon: 0.96; Loss: 47.5351\n",
            "Saving checkpoint...\n",
            "Episode: 50;  Timestep: 14440; 10 -Episode Mean reward: -20.3; Episode length: 305.7; Epsilon: 0.95; Loss: 63.7018\n",
            "Saving checkpoint...\n",
            "Episode: 60;  Timestep: 17497; 10 -Episode Mean reward: -20.3; Episode length: 305.7; Epsilon: 0.94; Loss: 49.4591\n",
            "Saving checkpoint...\n",
            "Episode: 70;  Timestep: 20356; 10 -Episode Mean reward: -20.6; Episode length: 285.9; Epsilon: 0.93; Loss: 43.5817\n",
            "Saving checkpoint...\n",
            "Episode: 80;  Timestep: 23389; 10 -Episode Mean reward: -20.5; Episode length: 303.3; Epsilon: 0.92; Loss: 42.4383\n",
            "Saving checkpoint...\n",
            "Episode: 90;  Timestep: 26526; 10 -Episode Mean reward: -20.1; Episode length: 313.7; Epsilon: 0.91; Loss: 37.0632\n",
            "Saving checkpoint...\n",
            "Episode: 100;  Timestep: 29572; 10 -Episode Mean reward: -20.7; Episode length: 304.6; Epsilon: 0.90; Loss: 32.4775\n",
            "Saving checkpoint...\n",
            "Episode: 110;  Timestep: 32533; 10 -Episode Mean reward: -20.6; Episode length: 296.1; Epsilon: 0.89; Loss: 32.7526\n",
            "Saving checkpoint...\n",
            "Episode: 120;  Timestep: 35604; 10 -Episode Mean reward: -20.4; Episode length: 307.1; Epsilon: 0.88; Loss: 32.0544\n",
            "Saving checkpoint...\n",
            "Episode: 130;  Timestep: 38715; 10 -Episode Mean reward: -20.5; Episode length: 311.1; Epsilon: 0.87; Loss: 32.5781\n",
            "Saving checkpoint...\n",
            "Episode: 140;  Timestep: 41700; 10 -Episode Mean reward: -20.6; Episode length: 298.5; Epsilon: 0.86; Loss: 30.8294\n",
            "Saving checkpoint...\n",
            "Episode: 150;  Timestep: 44863; 10 -Episode Mean reward: -20.7; Episode length: 316.3; Epsilon: 0.85; Loss: 33.7810\n",
            "Saving checkpoint...\n",
            "Episode: 160;  Timestep: 48052; 10 -Episode Mean reward: -20.8; Episode length: 318.9; Epsilon: 0.84; Loss: 35.3097\n",
            "Saving checkpoint...\n",
            "Episode: 170;  Timestep: 51254; 10 -Episode Mean reward: -20.5; Episode length: 320.2; Epsilon: 0.83; Loss: 34.5763\n",
            "Saving checkpoint...\n",
            "Episode: 180;  Timestep: 54876; 10 -Episode Mean reward: -19.8; Episode length: 362.2; Epsilon: 0.82; Loss: 39.2564\n",
            "Saving checkpoint...\n",
            "Episode: 190;  Timestep: 58195; 10 -Episode Mean reward: -20.1; Episode length: 331.9; Epsilon: 0.81; Loss: 38.8404\n",
            "Saving checkpoint...\n",
            "Episode: 200;  Timestep: 61544; 10 -Episode Mean reward: -20.3; Episode length: 334.9; Epsilon: 0.80; Loss: 40.7079\n",
            "Saving checkpoint...\n",
            "Episode: 210;  Timestep: 64912; 10 -Episode Mean reward: -20.1; Episode length: 336.8; Epsilon: 0.79; Loss: 42.8937\n",
            "Saving checkpoint...\n",
            "Episode: 220;  Timestep: 68314; 10 -Episode Mean reward: -20.3; Episode length: 340.2; Epsilon: 0.77; Loss: 42.3014\n",
            "Saving checkpoint...\n",
            "Episode: 230;  Timestep: 71812; 10 -Episode Mean reward: -20.2; Episode length: 349.8; Epsilon: 0.76; Loss: 42.7333\n",
            "Saving checkpoint...\n",
            "Episode: 240;  Timestep: 75322; 10 -Episode Mean reward: -20.5; Episode length: 351.0; Epsilon: 0.75; Loss: 44.9565\n",
            "Saving checkpoint...\n",
            "Episode: 250;  Timestep: 78496; 10 -Episode Mean reward: -20.5; Episode length: 317.4; Epsilon: 0.74; Loss: 38.8084\n",
            "Saving checkpoint...\n",
            "Episode: 260;  Timestep: 82034; 10 -Episode Mean reward: -19.7; Episode length: 353.8; Epsilon: 0.73; Loss: 42.4902\n",
            "Saving checkpoint...\n",
            "Episode: 270;  Timestep: 85446; 10 -Episode Mean reward: -20.3; Episode length: 341.2; Epsilon: 0.72; Loss: 41.7272\n",
            "Saving checkpoint...\n",
            "Episode: 280;  Timestep: 88804; 10 -Episode Mean reward: -20.5; Episode length: 335.8; Epsilon: 0.71; Loss: 41.8778\n",
            "Saving checkpoint...\n",
            "Episode: 290;  Timestep: 92156; 10 -Episode Mean reward: -20.7; Episode length: 335.2; Epsilon: 0.70; Loss: 38.5871\n",
            "Saving checkpoint...\n",
            "Episode: 300;  Timestep: 95655; 10 -Episode Mean reward: -19.8; Episode length: 349.9; Epsilon: 0.68; Loss: 40.1549\n",
            "Saving checkpoint...\n",
            "Episode: 310;  Timestep: 99006; 10 -Episode Mean reward: -20.7; Episode length: 335.1; Epsilon: 0.67; Loss: 37.4053\n",
            "Saving checkpoint...\n",
            "Episode: 320;  Timestep: 102689; 10 -Episode Mean reward: -20.1; Episode length: 368.3; Epsilon: 0.66; Loss: 41.3205\n",
            "Saving checkpoint...\n",
            "Episode: 330;  Timestep: 106435; 10 -Episode Mean reward: -20.4; Episode length: 374.6; Epsilon: 0.65; Loss: 38.8485\n",
            "Saving checkpoint...\n",
            "Episode: 340;  Timestep: 110505; 10 -Episode Mean reward: -19.4; Episode length: 407.0; Epsilon: 0.64; Loss: 42.1545\n",
            "Saving checkpoint...\n",
            "Episode: 350;  Timestep: 114207; 10 -Episode Mean reward: -20.1; Episode length: 370.2; Epsilon: 0.62; Loss: 37.9230\n",
            "Saving checkpoint...\n",
            "Episode: 360;  Timestep: 118042; 10 -Episode Mean reward: -19.6; Episode length: 383.5; Epsilon: 0.61; Loss: 35.7371\n",
            "Saving checkpoint...\n",
            "Episode: 370;  Timestep: 122162; 10 -Episode Mean reward: -20.0; Episode length: 412.0; Epsilon: 0.60; Loss: 39.6229\n",
            "Saving checkpoint...\n",
            "Episode: 380;  Timestep: 125909; 10 -Episode Mean reward: -20.2; Episode length: 374.7; Epsilon: 0.58; Loss: 35.7761\n",
            "Saving checkpoint...\n",
            "Episode: 390;  Timestep: 129830; 10 -Episode Mean reward: -19.7; Episode length: 392.1; Epsilon: 0.57; Loss: 38.8887\n",
            "Saving checkpoint...\n",
            "Episode: 400;  Timestep: 133731; 10 -Episode Mean reward: -20.0; Episode length: 390.1; Epsilon: 0.56; Loss: 38.7626\n",
            "Saving checkpoint...\n",
            "Episode: 410;  Timestep: 137921; 10 -Episode Mean reward: -19.4; Episode length: 419.0; Epsilon: 0.54; Loss: 42.2833\n",
            "Saving checkpoint...\n",
            "Episode: 420;  Timestep: 141889; 10 -Episode Mean reward: -19.4; Episode length: 396.8; Epsilon: 0.53; Loss: 38.1355\n",
            "Saving checkpoint...\n",
            "Episode: 430;  Timestep: 145990; 10 -Episode Mean reward: -20.0; Episode length: 410.1; Epsilon: 0.52; Loss: 40.8732\n",
            "Saving checkpoint...\n",
            "Episode: 440;  Timestep: 150403; 10 -Episode Mean reward: -19.8; Episode length: 441.3; Epsilon: 0.50; Loss: 43.3837\n",
            "Saving checkpoint...\n",
            "Episode: 450;  Timestep: 154547; 10 -Episode Mean reward: -19.4; Episode length: 414.4; Epsilon: 0.49; Loss: 39.9522\n",
            "Saving checkpoint...\n",
            "Episode: 460;  Timestep: 159040; 10 -Episode Mean reward: -19.3; Episode length: 449.3; Epsilon: 0.48; Loss: 46.5228\n",
            "Saving checkpoint...\n",
            "Episode: 470;  Timestep: 163495; 10 -Episode Mean reward: -18.7; Episode length: 445.5; Epsilon: 0.46; Loss: 44.8316\n",
            "Saving checkpoint...\n",
            "Episode: 480;  Timestep: 167532; 10 -Episode Mean reward: -19.0; Episode length: 403.7; Epsilon: 0.45; Loss: 40.5538\n",
            "Saving checkpoint...\n",
            "Episode: 490;  Timestep: 172144; 10 -Episode Mean reward: -19.0; Episode length: 461.2; Epsilon: 0.43; Loss: 46.5285\n",
            "Saving checkpoint...\n",
            "Episode: 500;  Timestep: 177064; 10 -Episode Mean reward: -18.5; Episode length: 492.0; Epsilon: 0.42; Loss: 49.4397\n",
            "Saving checkpoint...\n",
            "Episode: 510;  Timestep: 181649; 10 -Episode Mean reward: -19.0; Episode length: 458.5; Epsilon: 0.40; Loss: 50.4084\n",
            "Saving checkpoint...\n",
            "Episode: 520;  Timestep: 186574; 10 -Episode Mean reward: -17.6; Episode length: 492.5; Epsilon: 0.38; Loss: 55.9348\n",
            "Saving checkpoint...\n",
            "Episode: 530;  Timestep: 191588; 10 -Episode Mean reward: -17.5; Episode length: 501.4; Epsilon: 0.37; Loss: 59.2245\n",
            "Saving checkpoint...\n",
            "Episode: 540;  Timestep: 196626; 10 -Episode Mean reward: -17.0; Episode length: 503.8; Epsilon: 0.35; Loss: 60.8144\n",
            "Saving checkpoint...\n",
            "Episode: 550;  Timestep: 201860; 10 -Episode Mean reward: -18.2; Episode length: 523.4; Epsilon: 0.33; Loss: 62.0575\n",
            "Saving checkpoint...\n",
            "Episode: 560;  Timestep: 206960; 10 -Episode Mean reward: -17.8; Episode length: 510.0; Epsilon: 0.32; Loss: 65.4221\n",
            "Saving checkpoint...\n",
            "Episode: 570;  Timestep: 212328; 10 -Episode Mean reward: -17.4; Episode length: 536.8; Epsilon: 0.30; Loss: 71.3924\n",
            "Saving checkpoint...\n",
            "Episode: 580;  Timestep: 217218; 10 -Episode Mean reward: -18.8; Episode length: 489.0; Epsilon: 0.28; Loss: 67.0929\n",
            "Saving checkpoint...\n",
            "Episode: 590;  Timestep: 222637; 10 -Episode Mean reward: -16.7; Episode length: 541.9; Epsilon: 0.27; Loss: 71.9773\n",
            "Saving checkpoint...\n",
            "Episode: 600;  Timestep: 228443; 10 -Episode Mean reward: -17.0; Episode length: 580.6; Epsilon: 0.25; Loss: 78.1853\n",
            "Saving checkpoint...\n",
            "Episode: 610;  Timestep: 234656; 10 -Episode Mean reward: -16.7; Episode length: 621.3; Epsilon: 0.23; Loss: 84.6884\n",
            "Saving checkpoint...\n",
            "Episode: 620;  Timestep: 240325; 10 -Episode Mean reward: -17.4; Episode length: 566.9; Epsilon: 0.21; Loss: 79.3682\n",
            "Saving checkpoint...\n",
            "Episode: 630;  Timestep: 246658; 10 -Episode Mean reward: -16.1; Episode length: 633.3; Epsilon: 0.19; Loss: 87.2568\n",
            "Saving checkpoint...\n",
            "Episode: 640;  Timestep: 252719; 10 -Episode Mean reward: -17.5; Episode length: 606.1; Epsilon: 0.17; Loss: 83.0595\n",
            "Saving checkpoint...\n",
            "Episode: 650;  Timestep: 258493; 10 -Episode Mean reward: -17.2; Episode length: 577.4; Epsilon: 0.15; Loss: 78.5483\n",
            "Saving checkpoint...\n",
            "Episode: 660;  Timestep: 265681; 10 -Episode Mean reward: -15.2; Episode length: 718.8; Epsilon: 0.12; Loss: 97.4301\n",
            "Saving checkpoint...\n",
            "Episode: 670;  Timestep: 272699; 10 -Episode Mean reward: -15.0; Episode length: 701.8; Epsilon: 0.10; Loss: 88.0857\n",
            "Saving checkpoint...\n",
            "Episode: 680;  Timestep: 279257; 10 -Episode Mean reward: -16.3; Episode length: 655.8; Epsilon: 0.08; Loss: 86.9320\n",
            "Saving checkpoint...\n",
            "Episode: 690;  Timestep: 287027; 10 -Episode Mean reward: -14.6; Episode length: 777.0; Epsilon: 0.05; Loss: 107.1173\n",
            "Saving checkpoint...\n",
            "Episode: 700;  Timestep: 295709; 10 -Episode Mean reward: -14.1; Episode length: 868.2; Epsilon: 0.02; Loss: 111.7476\n",
            "Saving checkpoint...\n",
            "Episode: 710;  Timestep: 304732; 10 -Episode Mean reward: -13.3; Episode length: 902.3; Epsilon: 0.01; Loss: 105.1950\n",
            "Saving checkpoint...\n",
            "Episode: 720;  Timestep: 313845; 10 -Episode Mean reward: -14.1; Episode length: 911.3; Epsilon: 0.01; Loss: 103.0993\n",
            "Saving checkpoint...\n",
            "Episode: 730;  Timestep: 322069; 10 -Episode Mean reward: -15.1; Episode length: 822.4; Epsilon: 0.01; Loss: 91.9501\n",
            "Saving checkpoint...\n",
            "Episode: 740;  Timestep: 330808; 10 -Episode Mean reward: -14.1; Episode length: 873.9; Epsilon: 0.01; Loss: 104.1350\n",
            "Saving checkpoint...\n",
            "Episode: 750;  Timestep: 340591; 10 -Episode Mean reward: -12.0; Episode length: 978.3; Epsilon: 0.01; Loss: 115.6590\n",
            "Saving checkpoint...\n",
            "Episode: 760;  Timestep: 350506; 10 -Episode Mean reward: -12.1; Episode length: 991.5; Epsilon: 0.01; Loss: 111.0679\n",
            "Saving checkpoint...\n",
            "Episode: 770;  Timestep: 361377; 10 -Episode Mean reward: -10.1; Episode length: 1087.1; Epsilon: 0.01; Loss: 116.8897\n",
            "Saving checkpoint...\n",
            "Episode: 780;  Timestep: 370821; 10 -Episode Mean reward: -11.0; Episode length: 944.4; Epsilon: 0.01; Loss: 103.7730\n",
            "Saving checkpoint...\n",
            "Episode: 790;  Timestep: 382025; 10 -Episode Mean reward: -8.3; Episode length: 1120.4; Epsilon: 0.01; Loss: 110.1117\n",
            "Saving checkpoint...\n",
            "Episode: 800;  Timestep: 393539; 10 -Episode Mean reward: -7.2; Episode length: 1151.4; Epsilon: 0.01; Loss: 118.6105\n",
            "Saving checkpoint...\n",
            "Episode: 810;  Timestep: 403630; 10 -Episode Mean reward: -11.8; Episode length: 1009.1; Epsilon: 0.01; Loss: 101.6622\n",
            "Saving checkpoint...\n",
            "Episode: 820;  Timestep: 412933; 10 -Episode Mean reward: -13.4; Episode length: 930.3; Epsilon: 0.01; Loss: 95.0203\n",
            "Saving checkpoint...\n",
            "Episode: 830;  Timestep: 423461; 10 -Episode Mean reward: -7.8; Episode length: 1052.8; Epsilon: 0.01; Loss: 110.4280\n",
            "Saving checkpoint...\n",
            "Episode: 840;  Timestep: 433751; 10 -Episode Mean reward: -10.7; Episode length: 1029.0; Epsilon: 0.01; Loss: 106.0311\n",
            "Saving checkpoint...\n",
            "Episode: 850;  Timestep: 443849; 10 -Episode Mean reward: -10.6; Episode length: 1009.8; Epsilon: 0.01; Loss: 101.6420\n",
            "Saving checkpoint...\n",
            "Episode: 860;  Timestep: 454275; 10 -Episode Mean reward: -10.1; Episode length: 1042.6; Epsilon: 0.01; Loss: 101.0342\n",
            "Saving checkpoint...\n",
            "Episode: 870;  Timestep: 465014; 10 -Episode Mean reward: -9.1; Episode length: 1073.9; Epsilon: 0.01; Loss: 103.0409\n",
            "Saving checkpoint...\n",
            "Episode: 880;  Timestep: 476120; 10 -Episode Mean reward: -9.1; Episode length: 1110.6; Epsilon: 0.01; Loss: 108.9085\n",
            "Saving checkpoint...\n",
            "Episode: 890;  Timestep: 485731; 10 -Episode Mean reward: -11.4; Episode length: 961.1; Epsilon: 0.01; Loss: 93.1141\n",
            "Saving checkpoint...\n",
            "Episode: 900;  Timestep: 496312; 10 -Episode Mean reward: -11.8; Episode length: 1058.1; Epsilon: 0.01; Loss: 111.8556\n",
            "Saving checkpoint...\n",
            "Episode: 910;  Timestep: 507856; 10 -Episode Mean reward: -9.1; Episode length: 1154.4; Epsilon: 0.01; Loss: 112.7165\n",
            "Saving checkpoint...\n",
            "Episode: 920;  Timestep: 519236; 10 -Episode Mean reward: -9.3; Episode length: 1138.0; Epsilon: 0.01; Loss: 110.6380\n",
            "Saving checkpoint...\n",
            "Episode: 930;  Timestep: 530102; 10 -Episode Mean reward: -10.3; Episode length: 1086.6; Epsilon: 0.01; Loss: 104.0124\n",
            "Saving checkpoint...\n",
            "Episode: 940;  Timestep: 541631; 10 -Episode Mean reward: -7.7; Episode length: 1152.9; Epsilon: 0.01; Loss: 106.0248\n",
            "Saving checkpoint...\n",
            "Episode: 950;  Timestep: 552856; 10 -Episode Mean reward: -7.2; Episode length: 1122.5; Epsilon: 0.01; Loss: 102.4200\n",
            "Saving checkpoint...\n",
            "Episode: 960;  Timestep: 564443; 10 -Episode Mean reward: -7.6; Episode length: 1158.7; Epsilon: 0.01; Loss: 105.1895\n",
            "Saving checkpoint...\n",
            "Episode: 970;  Timestep: 575080; 10 -Episode Mean reward: -10.5; Episode length: 1063.7; Epsilon: 0.01; Loss: 97.6886\n",
            "Saving checkpoint...\n",
            "Episode: 980;  Timestep: 586123; 10 -Episode Mean reward: -9.7; Episode length: 1104.3; Epsilon: 0.01; Loss: 98.3833\n",
            "Saving checkpoint...\n",
            "Episode: 990;  Timestep: 597734; 10 -Episode Mean reward: -5.9; Episode length: 1161.1; Epsilon: 0.01; Loss: 106.2637\n",
            "Saving checkpoint...\n",
            "Episode: 1000;  Timestep: 609394; 10 -Episode Mean reward: -8.0; Episode length: 1166.0; Epsilon: 0.01; Loss: 102.6007\n",
            "Saving checkpoint...\n",
            "Episode: 1010;  Timestep: 621444; 10 -Episode Mean reward: -6.9; Episode length: 1205.0; Epsilon: 0.01; Loss: 103.7363\n",
            "Saving checkpoint...\n",
            "Episode: 1020;  Timestep: 633466; 10 -Episode Mean reward: -8.6; Episode length: 1202.2; Epsilon: 0.01; Loss: 101.5089\n",
            "Saving checkpoint...\n",
            "Episode: 1030;  Timestep: 644562; 10 -Episode Mean reward: -9.5; Episode length: 1109.6; Epsilon: 0.01; Loss: 95.5947\n",
            "Saving checkpoint...\n",
            "Episode: 1040;  Timestep: 656630; 10 -Episode Mean reward: -5.1; Episode length: 1206.8; Epsilon: 0.01; Loss: 103.4422\n",
            "Saving checkpoint...\n",
            "Episode: 1050;  Timestep: 667685; 10 -Episode Mean reward: -8.3; Episode length: 1105.5; Epsilon: 0.01; Loss: 94.8766\n",
            "Saving checkpoint...\n",
            "Episode: 1060;  Timestep: 677916; 10 -Episode Mean reward: -11.1; Episode length: 1023.1; Epsilon: 0.01; Loss: 95.3202\n",
            "Saving checkpoint...\n",
            "Episode: 1070;  Timestep: 689015; 10 -Episode Mean reward: -8.1; Episode length: 1109.9; Epsilon: 0.01; Loss: 106.3016\n",
            "Saving checkpoint...\n",
            "Episode: 1080;  Timestep: 699314; 10 -Episode Mean reward: -9.8; Episode length: 1029.9; Epsilon: 0.01; Loss: 96.7624\n",
            "Saving checkpoint...\n",
            "Episode: 1090;  Timestep: 710390; 10 -Episode Mean reward: -7.3; Episode length: 1107.6; Epsilon: 0.01; Loss: 107.0098\n",
            "Saving checkpoint...\n",
            "Episode: 1100;  Timestep: 721660; 10 -Episode Mean reward: -8.1; Episode length: 1127.0; Epsilon: 0.01; Loss: 108.5401\n",
            "Saving checkpoint...\n",
            "Episode: 1110;  Timestep: 732093; 10 -Episode Mean reward: -9.0; Episode length: 1043.3; Epsilon: 0.01; Loss: 97.1322\n",
            "Saving checkpoint...\n",
            "Episode: 1120;  Timestep: 742692; 10 -Episode Mean reward: -9.1; Episode length: 1059.9; Epsilon: 0.01; Loss: 107.3995\n",
            "Saving checkpoint...\n",
            "Episode: 1130;  Timestep: 753608; 10 -Episode Mean reward: -8.3; Episode length: 1091.6; Epsilon: 0.01; Loss: 111.4609\n",
            "Saving checkpoint...\n",
            "Episode: 1140;  Timestep: 765193; 10 -Episode Mean reward: -8.9; Episode length: 1158.5; Epsilon: 0.01; Loss: 107.4660\n",
            "Saving checkpoint...\n",
            "Episode: 1150;  Timestep: 776049; 10 -Episode Mean reward: -8.9; Episode length: 1085.6; Epsilon: 0.01; Loss: 94.6526\n",
            "Saving checkpoint...\n",
            "Episode: 1160;  Timestep: 787750; 10 -Episode Mean reward: -7.3; Episode length: 1170.1; Epsilon: 0.01; Loss: 104.6305\n",
            "Saving checkpoint...\n",
            "Episode: 1170;  Timestep: 798628; 10 -Episode Mean reward: -8.2; Episode length: 1087.8; Epsilon: 0.01; Loss: 96.9416\n",
            "Saving checkpoint...\n",
            "Episode: 1180;  Timestep: 810932; 10 -Episode Mean reward: -7.4; Episode length: 1230.4; Epsilon: 0.01; Loss: 111.5570\n",
            "Saving checkpoint...\n",
            "Episode: 1190;  Timestep: 822620; 10 -Episode Mean reward: -6.5; Episode length: 1168.8; Epsilon: 0.01; Loss: 103.2432\n",
            "Saving checkpoint...\n",
            "Episode: 1200;  Timestep: 834036; 10 -Episode Mean reward: -8.6; Episode length: 1141.6; Epsilon: 0.01; Loss: 97.2173\n",
            "Saving checkpoint...\n",
            "Episode: 1210;  Timestep: 846210; 10 -Episode Mean reward: -6.3; Episode length: 1217.4; Epsilon: 0.01; Loss: 107.3685\n",
            "Saving checkpoint...\n",
            "Episode: 1220;  Timestep: 857570; 10 -Episode Mean reward: -6.6; Episode length: 1136.0; Epsilon: 0.01; Loss: 99.3842\n",
            "Saving checkpoint...\n",
            "Episode: 1230;  Timestep: 868606; 10 -Episode Mean reward: -1.4; Episode length: 1103.6; Epsilon: 0.01; Loss: 93.4694\n",
            "Saving checkpoint...\n",
            "Episode: 1240;  Timestep: 880314; 10 -Episode Mean reward: -5.9; Episode length: 1170.8; Epsilon: 0.01; Loss: 107.0432\n",
            "Saving checkpoint...\n",
            "Episode: 1250;  Timestep: 891057; 10 -Episode Mean reward: -4.4; Episode length: 1074.3; Epsilon: 0.01; Loss: 100.9626\n",
            "Saving checkpoint...\n",
            "Episode: 1260;  Timestep: 902237; 10 -Episode Mean reward: -5.5; Episode length: 1118.0; Epsilon: 0.01; Loss: 111.3795\n",
            "Saving checkpoint...\n",
            "Episode: 1270;  Timestep: 913051; 10 -Episode Mean reward: -5.6; Episode length: 1081.4; Epsilon: 0.01; Loss: 108.9683\n",
            "Saving checkpoint...\n",
            "Episode: 1280;  Timestep: 925231; 10 -Episode Mean reward: -1.8; Episode length: 1218.0; Epsilon: 0.01; Loss: 117.3344\n",
            "Saving checkpoint...\n",
            "Episode: 1290;  Timestep: 937375; 10 -Episode Mean reward: -3.5; Episode length: 1214.4; Epsilon: 0.01; Loss: 109.1943\n",
            "Saving checkpoint...\n",
            "Episode: 1300;  Timestep: 948740; 10 -Episode Mean reward: -5.2; Episode length: 1136.5; Epsilon: 0.01; Loss: 110.5999\n",
            "Saving checkpoint...\n",
            "Episode: 1310;  Timestep: 959859; 10 -Episode Mean reward: -4.2; Episode length: 1111.9; Epsilon: 0.01; Loss: 104.5867\n",
            "Saving checkpoint...\n",
            "Episode: 1320;  Timestep: 970942; 10 -Episode Mean reward: -4.3; Episode length: 1108.3; Epsilon: 0.01; Loss: 105.1839\n",
            "Saving checkpoint...\n",
            "Episode: 1330;  Timestep: 982577; 10 -Episode Mean reward: -6.7; Episode length: 1163.5; Epsilon: 0.01; Loss: 109.9323\n",
            "Saving checkpoint...\n",
            "Episode: 1340;  Timestep: 994508; 10 -Episode Mean reward: -5.4; Episode length: 1193.1; Epsilon: 0.01; Loss: 116.7517\n",
            "Saving checkpoint...\n",
            "Episode: 1350;  Timestep: 1005381; 10 -Episode Mean reward: -8.5; Episode length: 1087.3; Epsilon: 0.01; Loss: 107.4415\n",
            "Saving checkpoint...\n",
            "Episode: 1360;  Timestep: 1015926; 10 -Episode Mean reward: -4.7; Episode length: 1054.5; Epsilon: 0.01; Loss: 102.6293\n",
            "Saving checkpoint...\n",
            "Episode: 1370;  Timestep: 1026671; 10 -Episode Mean reward: -6.3; Episode length: 1074.5; Epsilon: 0.01; Loss: 102.1489\n",
            "Saving checkpoint...\n",
            "Episode: 1380;  Timestep: 1038116; 10 -Episode Mean reward: -0.8; Episode length: 1144.5; Epsilon: 0.01; Loss: 111.1582\n",
            "Saving checkpoint...\n",
            "Episode: 1390;  Timestep: 1049395; 10 -Episode Mean reward: -5.1; Episode length: 1127.9; Epsilon: 0.01; Loss: 106.2067\n",
            "Saving checkpoint...\n",
            "Episode: 1400;  Timestep: 1060995; 10 -Episode Mean reward: -5.6; Episode length: 1160.0; Epsilon: 0.01; Loss: 105.5952\n",
            "Saving checkpoint...\n",
            "Episode: 1410;  Timestep: 1072721; 10 -Episode Mean reward: -3.3; Episode length: 1172.6; Epsilon: 0.01; Loss: 108.9899\n",
            "Saving checkpoint...\n",
            "Episode: 1420;  Timestep: 1084908; 10 -Episode Mean reward: -4.9; Episode length: 1218.7; Epsilon: 0.01; Loss: 112.1357\n",
            "Saving checkpoint...\n",
            "Episode: 1430;  Timestep: 1095161; 10 -Episode Mean reward: -10.8; Episode length: 1025.3; Epsilon: 0.01; Loss: 94.6581\n",
            "Saving checkpoint...\n",
            "Episode: 1440;  Timestep: 1106820; 10 -Episode Mean reward: -6.0; Episode length: 1165.9; Epsilon: 0.01; Loss: 107.1544\n",
            "Saving checkpoint...\n",
            "Episode: 1450;  Timestep: 1118042; 10 -Episode Mean reward: -7.3; Episode length: 1122.2; Epsilon: 0.01; Loss: 106.1560\n",
            "Saving checkpoint...\n",
            "Episode: 1460;  Timestep: 1129647; 10 -Episode Mean reward: -4.5; Episode length: 1160.5; Epsilon: 0.01; Loss: 107.0496\n",
            "Saving checkpoint...\n",
            "Episode: 1470;  Timestep: 1140681; 10 -Episode Mean reward: -6.5; Episode length: 1103.4; Epsilon: 0.01; Loss: 101.1576\n",
            "Saving checkpoint...\n",
            "Episode: 1480;  Timestep: 1150990; 10 -Episode Mean reward: -10.5; Episode length: 1030.9; Epsilon: 0.01; Loss: 97.6302\n",
            "Saving checkpoint...\n",
            "Episode: 1490;  Timestep: 1162848; 10 -Episode Mean reward: -4.0; Episode length: 1185.8; Epsilon: 0.01; Loss: 118.6170\n",
            "Saving checkpoint...\n",
            "Episode: 1500;  Timestep: 1174431; 10 -Episode Mean reward: -3.2; Episode length: 1158.3; Epsilon: 0.01; Loss: 106.2777\n",
            "Saving checkpoint...\n",
            "Episode: 1510;  Timestep: 1185906; 10 -Episode Mean reward: -8.0; Episode length: 1147.5; Epsilon: 0.01; Loss: 105.5245\n",
            "Saving checkpoint...\n",
            "Episode: 1520;  Timestep: 1197371; 10 -Episode Mean reward: -5.6; Episode length: 1146.5; Epsilon: 0.01; Loss: 107.7939\n",
            "Saving checkpoint...\n",
            "Episode: 1530;  Timestep: 1209781; 10 -Episode Mean reward: -3.3; Episode length: 1241.0; Epsilon: 0.01; Loss: 113.5885\n",
            "Saving checkpoint...\n",
            "Episode: 1540;  Timestep: 1221449; 10 -Episode Mean reward: -4.4; Episode length: 1166.8; Epsilon: 0.01; Loss: 112.1547\n",
            "Saving checkpoint...\n",
            "Episode: 1550;  Timestep: 1233471; 10 -Episode Mean reward: -7.0; Episode length: 1202.2; Epsilon: 0.01; Loss: 113.1538\n",
            "Saving checkpoint...\n",
            "Episode: 1560;  Timestep: 1244571; 10 -Episode Mean reward: -5.6; Episode length: 1110.0; Epsilon: 0.01; Loss: 105.6663\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PLOTTING Mean n-Episode Reward vs timestep (or Episodes)"
      ],
      "metadata": {
        "id": "_N5XH8Kb8_nu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "episode_num, epochs, mean_reward = load_Reward_Seq(RL_A4_path)"
      ],
      "metadata": {
        "id": "OPlkfjtdkC-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for episodes, reward in zip(episode_num, mean_reward):\n",
        "# plot rewards\n",
        "plt.plot(epochs, mean_reward)\n",
        "plt.xlabel('#Timestep/epochs')\n",
        "plt.ylabel('Episode Reward')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "wJ2YEAq6qCKN",
        "outputId": "ddf69bf3-6397-4413-8b68-21d946a90b73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Episode Reward')"
            ]
          },
          "metadata": {},
          "execution_count": 34
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEICAYAAABxiqLiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOy9d3hkZ3m/fz/TR9KM+kpabfU2e93WZl1jOzY2YKoxCWACCQkkTgj8kpCEFvIjpJBQAgQSAjgJCS2YanCwscHGuLddly3e3lVXXZre3u8fp+jMaEYalZFGu+99XXNp5pSZV+18ztNFKYVGo9FoNLPFtdQL0Gg0Gs3yRAuIRqPRaOaEFhCNRqPRzAktIBqNRqOZE1pANBqNRjMntIBoNBqNZk5UrYCIyM0ickBEDovIh4vs94vId839T4vIusVfpUaj0Zy9eJZ6AcUQETfwJeAVQBfwrIjcrZR6yXHYu4ERpdRGEbkN+BTw1unet6WlRa1bt65Cq9ZoNJozk507dw4qpVoLt1elgACXA4eVUkcBRORO4BbAKSC3AB83n/8A+DcRETVNZeS6devYsWNHZVas0Wg0ZygicqLY9mp1YXUCpxyvu8xtRY9RSmWAMaC58I1E5HYR2SEiOwYGBiq0XI1Gozn7qFYBWTCUUncopbYrpba3tk6xwDQajUYzR6pVQLqB1Y7Xq8xtRY8REQ9QDwwtyuo0Go1GU7UC8iywSUTWi4gPuA24u+CYu4F3ms9/E/jldPEPjUaj0SwsVRlEV0plROR9wP2AG/iaUmqviPwdsEMpdTfwX8A3ReQwMIwhMhqNRqNZJKpSQACUUvcC9xZs+5jjeQJ482KvS6PRaDQG1erC0mg0Gk2VowVEo9FoTCLJDHc937XUy1g2aAHRaDQak3t39/L+777IiaHoUi9lWaAFRKPRaEzG42kARmLpJV7J8kALiEaj0ZhEk1lgUkg006MFRKPRaEyiqQwAY1pAykILiEaj0ZhMJAwBGU9oASkHLSAajWZReHBfP7u6Rpd6GdMSTWoLZDZoAdFoNIvC3/30Jb78qyNLvYxp0QIyO7SAaDSaRWEikan6C3PEFJDxeGaJV7I80AKi0WgWhcgyEBAriK6zsMpDC4hGo6k4yUyWVDY3p+D01584zuHTExVY1VQiCe3Cmg1aQDQaTcWx6ivGZlmgl8xk+Zu79/I/TxyvwKqmErHqQHQWVlloAdFoNBXHCk5PJDPkcuWP7RmJGhfyA32LY4HoIPrs0AKi0WgqjhWcVsoQkXIZjqYA2N83QaXnxWVzinhaV6LPBi0gGo2m4kQcojGbi/NIzBCQiUSGnrHEgq/LiRVAD3rdjCcyFResM4GqExAR+YyI7BeRXSJyl4g0lDjuuIjsFpEXRGTHYq9To9GUj1NAZuMeGjItEIADfeMLuqZCLPdVR0OAbE7lrVlTnKoTEOAXwAVKqYuAg8BHpjn2BqXUNqXU9sVZmkajmQtWdhPMLkA94hCQ/RWOg1hr7GwIAjCe0AIyE1UnIEqpnyulrN/cU8CqpVyPRqOZP9E5urCsGEh7OFDxQLplcaysNwRkthljZyNVJyAFvAv4WYl9Cvi5iOwUkdtLvYGI3C4iO0Rkx8DAQEUWqdFopic/BjK7IHpDjZetK8Ps762sgFipxh0NAUBnYpWDZyk+VEQeANqL7PqoUuon5jEfBTLAt0u8zTVKqW4RWQH8QkT2K6UeKTxIKXUHcAfA9u3bdVRMo1kC5hoDGY6laKrxsaU9xCMHB0hlcvg8lbnvtS0Q24WlBWQmlkRAlFI3TbdfRH4XeB1woyqRCqGU6ja/nhaRu4DLgSkCotFolp5IIkPA6yKVmV01+kg0RVOtj3PbQ2RyiqODEc5tD1dkjZabzYqBaAtkZqrOhSUiNwMfBN6glIqVOKZWRELWc+CVwJ7FW6VGo5kN0VSGUMBLOOidnQUSTdFY67NFo5JxkCkWiBaQGak6AQH+DQhhuKVeEJGvAIjIShG51zymDXhMRF4EngHuUUrdtzTL1Wg0MzGRyBDyewgHvLMOojfV+DintRavW9hXwTiIJSDt4QAiWkDKYUlcWNOhlNpYYnsP8Brz+VHg4sVcl0ajmTvRZIZav3G5KdcCUUoxEkvRVOfD63axobWuorUg0WQGt0sIeF2EA7OzlM5WqtEC0Wg0ZxjRZJZav5tw0FN2fcVEMkM6q2iq8QGwpT1UURdWNJmhzu9BRGa1zoUmm1N88AcvsvPEyJJ8/mzQAqLRaCrORDJDnd9L/SxiIFYRYWOtISDntofpGUtUrD4jksxSZ1pJ063zkz/bz317+iqyBoCnjg7xvR1d/OrA6ZLHHD4d4b3ffo54KluxdZSDFhCNRlNxjLt796xiIFYRYbMtICEADvRXxgqJJNPU+t1AaQHJ5RT/9dhR7nz2ZEXWAHDX892AETcqxS/393PP7l4ePzxYsXWUgxYQjUZTcSLJDHUBz6yysIYLLJAtloBUKA5iuNkMC6SU0PVPJEhnFS/1VGYNiXTWtm6mS3c+OWwkqD5+RAuIRqM5w4mYQfT6oJdkJkciPbPrxRIQKwbSUR8gFPBwsD9SsTXO5MLqGokDcHoiycBEcsHX8MC+fiLJDB6XTGuBnBw21vHE4aEFX8Ns0AKi0WgqSiqTI5XJmWm8xgW6nGJCq5V7Y60XABFh04o6DlVovG00maHWZ1ogJQVksjTtpd6Ft0J+/Hw3bWE/21Y3MDHNz+iUaYEc6J+oiJCVixYQjUZTUawK71q/4cKC8vphDUVT+Nwu2yoA2LQixKEKWSBR080GlLSUTpl3/sCCu7GGoyl+dWCAW7Z10lDjLWmBZHOKrpEY12xsAeCJJXRjaQHRaDQVxSrQq3MISDlxkJFoisZaLyJib9vUVsdQNMVQZOHvuiccLixb6AqsgK6RGCtCfjobggtugdyzu5dMTvHGbZ2EAt6SVlrvWJx0VvHqC9sJBzxL6sbSAqLRaCqKU0DqS1yYizEcTdNoxj8sNrUZgfTDpxfWClFKmcWORhaW7WqLFwpInFWNQbauDPNSz9iCruH/XuhhS1uI8zpChAKekhaIFUBf31zLVRuaeezw4JJNT9QCotFoKkrE6cIKWC6s8mIgzXUFArKiDoCDCywgiXSOnMLOwqq3LaX8i/ipkRirGms4f2WYo4NRYqmFKzY8NRLjwlX1iIgtIMWEwYp/rG6q4dc2ttA9GrdFZbHRAqLRaCqKbYEEHBZIGQIyHE1NsUA66gPU+T0cdtSC/HRXD6/94qN88cFD9sV1zmssEBDnOjPZHL2jCVY3BdnaEUaphZ2SaATxDQsoFPCSzSniRbLVTgzF8LiEjvoAV28w4iCPL5EbSwuIRqOpKNaoWCMGUn4/rGGzlbsTEWHjirq8VN47nznF4dMRPveLg1z76Yf43C8OznqN0QIBKRYD6Z9IkskpVjXWsHWl0R14IQPp8XSWGvPzQ6YLrZgb6+RwjFWNQTxuFxtaa2kPB5asHkQLiEajqSjOi7Pf4ybgddl9pvZ0j/H6f30sb/Y5GHf7Y/H0FAEBzFReQ0DiqSzPHB/mHVeu5fEPv5zzOsI8emj2k0edbjZwurAmBcSyblY1BulsCFIf9C5YID2VyZHOKtsCsVx9xVJ5Tw3HWN1UAxiCevXGZp48MrQkcRAtIBqNpqIUXpzDAa/dz+qh/afZ3T3GYwUtOUbM/cUEZHNbiMFIkpFoiqePDZHK5LhucyudDUEuWBmmeyQ+5Zxy11jnWCPkz0W3ighXN9YgImztCM/KAvmdrz3DPbt6i+6zYilBX74FUhiDAcMCWWMKCMDWjjDD0dSsRgUvFFpANBpNRSkWX7BcQ1YM4amj+T58u4iwZqqAbGwzAumHTkd49NAgPo+Ly9c1AdDZGOT0RJJkZnZNBqMFIufzuAh63XkurK6RGCKTM9O3rgyzv2+cbG7mO/+JRJpHDg7wy/3FGyTGzKaIzhiIdZ6T8USakVg6T0A66o0BWD1jsxfO+aIFRKPRVJRoMkPQ68btMuo5nFXe+82+Vk8WCEhhI0UnVibWodMTPHJwgCvWNxE0L7zWONq+scSM6/rrH++24yWTIue29xe2Mzk1HKctFMDvMY7Z2hEmkc5xbHDmjLD+caNu5eRwtOj+SQskP424MAZycshwo+UJiCloPaNaQBCRj4tItzmN8AUReU2J424WkQMiclhEPrzY69RoNOURcVR4w6QFkkhnOTYYpaHGy9GBKKfHJy/6hY0UnXQ2BKn1uXn04CCHTke4dlPL5L5GQ0DKcWM9dmiQ+83GhdGkaQE4qt7DQU+egHSNxFjdFLRfW4H0vWW4sazvrVS67aQFYrmwLAukII3YkcJrYYlmTxmiudBUnYCYfF4ptc183Fu4U0TcwJeAVwNbgbeJyNbFXqRGo5kZ55wNMO6ux+JpDp+OkFNw22VrAHjq2LB9jN1IsYiAWJlYP3/JuPhft7nV3mddTLvLuBsfjac5Ohghnc1NycICU+gccQWjiHDywr1xRR0+t6usMbt9poD0jyeLNpK0BKzGdmFZFki+C8sSoDXNk+toqfPjcQm92gIpm8uBw0qpo0qpFHAncMsSr0mj0RQhkkgXvTBb8Y/fuLSTkN/Dk0cm3VhWVlZDjbfoe25cESKnYEXIzxazOh2MeIDIzAKSyynG42nSWcWJoRgTVgzEl79OywJJZ3P0jhlV6BZet4uGGi+jsfwMsmJYLiwoboXE08bnW2m8NT7D5TfFhTUco7HGawf5AdwuoS0c0C4sB+8TkV0i8jURaSyyvxM45XjdZW6bgojcLiI7RGTHwMDs0/s0mqXin362jw/+4MWlXsa8scbZWoRNF9a+3nH8HhfntNZx+fomnnbEQYaiKUJm2m8xNpuB9Gs3teb1yvJ5XKwI+Wd0YU0kM1ix70P9E0STGWp8blyuyfeqD/roH0+QzuboG0uQU0YGlpMan9t2P01Hv8M9d2JoqoDYLjTTArGq0QtbvhRmYFl0NgTPHheWiDwgInuKPG4BvgxsALYBvcBn5/NZSqk7lFLblVLbW1tbZz5Bo6kSdh4f4WmHW2e54mxSCMadvVKw48QIm9tCuF3Clec0c3Qwal9oR2KpovEPi83mcKnrNrdM2beyIThjRpKzwvxgf8Tsg+XJO+a1F7UzFE3xjSdPcGpksgbESdDnKUtATk8kaDHbspwYmhpIt0bTWkF0oGg/rJOOGhAnHQ0Bes+WLCyl1E1KqQuKPH6ilOpXSmWVUjngPzDcVYV0A6sdr1eZ2zSaM4axeJrhyMzukWonWiAglvtlT/eYPWXwqg3NwGQ673B0egG5blMr//zmi3nNhR1T9nU2BGe0QEYd9R2HTk/kDZOyuGHLCq7d1MIXHjjIri6jceKqIhaI5X6ajr6xBJvbjCaJxVxY0dRUF1rI782LgWSyObpH4kUtkI76oGEllZFSvJBUnQtLRJx/EbcCe4oc9iywSUTWi4gPuA24ezHWp9EsFqPxNBPJTFnT+6qZSMHdvdUmJJtT9pzz8zrChAOePAEplsJr4XYJv/myVXjdUy9hnY1BekYnL6bxVJZrP/1L7t09WcQ3GjeEORzwcMi0QAoFRET42Ou2Ek1l+eKDh3A5akAsyndhJWkPB1jbXFPUhRUrYYGMOyyQ3rEEmZxibfNUAVnZECCdVQxWoM39dFSdgACfFpHdIrILuAF4P4CIrBSRewGUUhngfcD9wD7ge0qpvUu1YI2mElgB3OHo8rZCCtN4rX5YAOe2G6mwbpdwxTnN/Pj5Hj7yo930jMaLFhGWQ2dDkFQ2Z19Md3WNcmo4blsRMPmz3b6uiaODEUbj6bw4jcWmthDvuGINsVSWjvrgFMEKet22+6kUSilOTyRYEQ6wtqm2qAUSS2VwuwS/Z/L9Q4H8oVLFUngtVtZPTeVVSlW8vUnVCYhS6reVUhcqpS5SSr1BKdVrbu9RSr3Gcdy9SqnNSqkNSqlPLN2KNZqFJ5HOksrkABhaxm4sa5xtXUF2k4XlwgL42Ou28uoL2vnx892MxNK0hvxz+szCVN4dJ0YA8oZQWS6s7esaSWcVB/omplggFn9202bqg96irqNyLJDhaIp0VtEW9rOmuYaukdiU6vVoMkuN152XEBAOePJcWJbwFAbyYdIycqbyvvWrT3HNpx7isz8/wPHB4gWM86X4T0yj0SwpTh/9YHTpZl7Pl6ijlbuFFQNpqfPlicTqpho+99Zt/N0bMzx6cIDtZnuS2WIXE47GuWRNI89ZAuKw5CwL5DLzM2Kp7JQgukVjrY9v//4VedaBRTlBdCuFty0cIOh1k84qekbjeZZEPJWlpsACCgfzLZDu0Thus417IZYFYolmNJnh2RPDdIQDfOmhw/zrLw/zlXdcys0XTI0ZzQctIBpNhcnmlN3Go1ycFdDL2QIpbKQIkzEQp/XhpM7v4dVFguPlsrJhshpdKcXOk1MtkLF4Gr/HxflmNXnhGgu5oLO+6PYan5v4DEOl+icMt1JbOGDXtRRmU0VTmbwAOlhZWGmUUogI3aNx2sMBPEXiPg01XoJeN72mC2tf7zhKwd+/8QIu6KznR891c9WGqRlr86XqXFgazZnEWCzNRR+/n4cPzq4GyVmcVon534uFJSAhvzO7yGNevItflOdLOOAlFPDQMxrn6GCU0Vgan9vFoEOIx2JpGmq81Pg8dmpuaBoBKUWNz00snc2LNSilSGdz9murjUlb2M/a5lpgai1IPJXNC6CDISA5BVHTwukeibOyYar1AUbA35nKu6fbiPdc0FlPWzjAe67fkOc6XCi0gGg0FaRrNEY0lbXdKOWSZ4Es4yB6YZdbAJdL+M7tV/LH12+o2Od2NgTpHo2z0/y5X7OphcFI0r7Qj8ZTNASNIP1ms5J9OgukFEGfG6WMkbgWX3n4KK/8/CP2Z/WNGTcAK0IB2sMBfG4XJwqaKha3QPI78vaMxW3rqhgr64N0jxpitbt7nJY6PyvmGEcqFy0gGk0FGYka//zFisemwxIQlyxvF9ZEkRgIwKVrGmmYY5ZVOaxqDNI1Emfn8REaarxctq6JZCZn382PxtL2Hfkms6p9LgJS4zWsBuds9CMDEY4NRm0ro38iQXOtD5/HhdslrGoK2l11LUpZIGA0VMzmFH1jiWkFpKM+YAfR9/aMcUFnOC8oXwm0gGg0FcSaa3FilrO6LQFZ01TD0DIMolstOIo1KVwMVjYE6RmNs/PkCJeuabSD9ZY7cCyept6MR2xaETLXWLxtynTUmFaDM5BuWQwvdo0ChgtrRXjS9bS2aWotSDSVnZJG7LRABiaSpLPKzjArxsqGIAORJBOJNIdOR7iwRNxmIdECotFUEEtACu84Z2IsnsYlsKa5dtlZII8dGuTiv/05P3quy56HPpe7+/nQ2RBkPJHh8OkIL1vbSLPZRsSKg4zFJy2QrR1GIL25dvbuHstqiDuKPa0Ovs+fNASkfzxJW3jyvdc2G7UgzrhJPJUl6J0aRAcYT2Ts7KrpBSSAUvDwwQGyOVWxGJMTLSAaTQWxXFhD0ZQdUC6H0ViacNBLa52/qoPoY7H0lGK1l3rHUAo+8INd3GNWfy+2BdLp6Fl16ZpGWmqnWiANloCsDPPD91zFDeeumPXnWO3X8yyQZL4F0jeeoC00aYGsaaohkszkFYhGU5kpFoiV7jweT9uddqd3YRn7fr63H4ALOsMlj10otIBoNBVkxJFNNZs4iHWBa6nzMRhNVbyieC70jMa5/B8fsEXC4tRwnFDAw4Wd9Tx6yJh1XuubvXtoPlgXWrdLuHh1PS2hSQsklckRS2XzspJetrZp1qnWMGmBOGMgVu3G3p5x4qksg5EkbfX5AgJwytGvK5bM2u4wC+dUwkkBKZ6FZewzvueH9p+mscY7rbWyUGgB0WgqSL6AlO/GslwszXU+UpncrKyXxeK+PX0kMzn2dOdP5Ds5HGNtcw3/83uXsaUtRH3QW7R2oZKsMi+eWzvC1Pg89mCqoUjSji+VmjUyG6yLvrOdyXg8TYv5e3vk0ABKkefCsppEWqna6WyOVDZnWzMWzqmE3aNxwgGPva0YlrhMJDNc0Flf8QA6aAHRaCrKSCzNhtbiuf/TMRpPU1/js/3y1RgHuW+vMRHwVEGCwKkRY2ZFQ42P77/nKr73h1ct+tpa6vyEAh6uPMeoNPd73IQCHoaiKcbMRor1C5AFVujCUkoxkchwzUajaO9+82fkdGHVBydjG85zCwUk4HXhcQkTCcOFNZ37yjjfY1tVixH/AC0gGk1FGY2lWNVYQ3Otj5MFuf/TzW8Yd1ggQNVlYg1MJHn2uDGrxFnTkMspuoYn23SEA96SFeeVxOUS7n7fNfzZTZvtbS11fgYjSbtNzEIU1lkXfcsCiaezZHKKze0hWkN+Htx3GjCq0C2sSnzLEorbAjK1G7A1E6R7NFGWS8pqc7IY8Q/QAqLRVJThaIqmWh9rCtp4/3xvH1d/8pccGYgUPW80lqI+6KlaC+QXL/WjFFy+rikvw6x/IkEqmyva8G+xWd9Sm5f91VLnY9DpwloQAbHSeA1rwop/hANetq1usD+rrX7SheUMjoNjFkiRNGKjI2+a7pFYXmJAKSyRWYwUXtACotFUlFGzZUZh7v8D+4wL8C4zU8eJUorxRIaGoM9hgVSXgNy3t4+1zTW88vw2xhMZ259/atiwqop1rl1qmmv9DEVSFbFAYmYar1UDEg4aAgJGIN+ZIhzwuvF5XHatjD2N0FtMQDz0jCUYT2RmdGEBbFhRR0udf9F+/iVz60Tkz6c7USn1uYVfjkYzSddIrGTzuKWmZzROR31g2kClFfxuqvERDnj5yYs9JDNZfG4Xj5nZSfv7JqacF0kalcf1QW9e8LdaGIuneeLwIO++dr19oTo5HKOhxjfZcrwaBaTOxzPHUwsaRPd7XIhMisCYWQMSCnhsAWmt80/J8KoPeictkCLtXizCAS8v9RpJCuUIyJ/euInf+7V1ixJAh+ktkJD52A68B+g0H38EXFqpBYnId0XkBfNxXEReKHHccXPw1AsisqNS69EsDf3jCV7+zw/zkxd6lnopU3ju5AjXfOqX/O3/vTTtcdZdeUOtj7XNNSgFXSNGgz9r8M+BIgLivEMOeN2E/J68RoDTEUtNpnzOl4lE2p5R7uTBff1kcoqbz29njTkdz7KuTg3HEJm+4G2paK7zMxJLMRRNIsK0GU3lIiLUeCdngtgWSMDDhavqEcnPwLIIBzx2waFlvRQG0cEQIkvwOqdJ4bWo9XvsepDFoKQFopT6WwAReQS4VCk1Yb7+OHBPpRaklHqr9VxEPguMTXP4DUqpwUqtRbN0PHZokFQ2R1+RC9hSopTiE/fsI6fgf544znWbW3j5uW1Fjx0xhaCxxmsHN08Oxex6kJetbeRgEQGxLhhWq43mOl/ZLqx/++VhvvXUCZ76qxunBGVny9//9CUePzzEox+8AZfjDvq+PX101Ae4eFWDXYFtWR6nhmN0hAP4iszOWGpa6nwoBccHY4T8njnVfRTDORNk3BEDCQe8XLCyng2tdVPOCQe9tgsrliweRId8ketsqD6rrpzfchvg/OtNmdsqihg22FuA71T6szTVxxNHjNnY1Vb/cN+ePnaeGOFv33A+53WE+cD3d3F6orjIWZXGTTU+1jRZqbxRHjs8yNrmGm46r42esURe511wCEjQEpDyq9GPDUYZT2Ts7J/58MKpUbpH4+zunryHG0+kefjgAK86vx2XS6j1e2ip89uB9FMjsap0X4GRhQVGs8OFbOTonAliWSDWhf8b77qcv3vjBVPOqQ967d+zFYAvZYEAeFwy5wmNlaQcAfkG8IyIfNy0Pp4G/qeSizK5FuhXSh0qsV8BPxeRnSJye6k3EZHbRWSHiOwYGJjdTAbN0qCU4skjhmEZSVSPgKQyOT553342t9Xx9ivW8MXbthFNZfiL771ILje1Utx2YdX4aKnzUeNzc2QgypNHhrhmYwvnmumtB/vzrZBCH31zra/sLCxroNDdL87P9ZfMZDkyYFhKD+6fFKP/e7GHZCbHmy7ttLetaQraFkjhoKRqotmMJx0djC7obAznWFs7C8us9Wis9RVt4xIOTMZAStWBGMcZ57bXBxbMYlpIphUQ0wr4BvB7wIj5+D2l1D/N50NF5AER2VPkcYvjsLcxvfVxjVLqUuDVwHtF5LpiByml7lBKbVdKbW9tbZ3PsjWLxPGhmB0jqCYL5FtPneDEUIy/es15eNwuNrWF+NDN5/LooUGePzU1m8pyYTXV+hAR1jTVcO/uXqKpLNduarXrIwoD6YVZQs11/rLrQKzakl8dOM1YLD3D0aU5fDpCNqfwuIQH9/Xb27+/o4stbaG8NFGrOWAinaV/PFmVGVhg/BzBuBFYiAC6RdDntl154/E0bpcUzahyEg56bHfXZBpvaRdWOQH0pWBaAVFGA557lVLPKaW+YD6en++HKqVuUkpdUOTxEwAR8QBvAr47zXt0m19PA3cBl893XZrq4AnT+gj5PXkzoZ30jSXsSW+LxbeeOsHl65v49c2TNyJXbWi211PIiG2BGBeBtc01DEVTuMQ4r6M+QCjg4UBffiuQKS6sWh/D0VRRK8dJOpvj9ESS67e0ks4q7tvbO+3x07Gv1xC1Wy/pZG/POH1jCQ71T/DCqVHevH1VXpbP6qYaesbiHBs0LJZqFZCWukm3VbiCFkg44JkxC8pyYSmliKeyiFB05rrlwlq1HAXE5DkRuaziK8nnJmC/Uqqr2E4RqRWRkPUceCWwZxHXp6kgTxweoj0c4NyOEJFk8bvoD/zgRT7wg12LtialFF2jcS5Z3ZB3cbD86oNFYhQj0RRBr5uAeTe6zhxnevHqBuqDXkSELW2hKZlYY3FjBKt1F9tc5yOnjPYm0zEwkUQpeMXWNtY118zLjbW/dxy/x8W7r10PwIP7+/n+zi48LuHWSzrzjl3bZGSYPWnGrVY3VefFrj7oxWO6gRaiiNAi6HUG0dNlZXeFA16yOUUslSWazFLrKy46ltAtSwvE5ArgSRE5IiK7zNTZSv/n3kaB+0pEVorIvebLNuAxEXkReAa4Ryl1X4XXpFkEcjnFk3GUPRwAACAASURBVEeHuHpjM6GAt6QLazCSsmckLAbDUaOLa3t9fiplY40Pt0uKCshwLGXXcQB2yuu1myYtmC3thoA4u+2OxVOETYGBSdfLTIF0K/6xsj7IGy5eyZNHhkoG+Gdif98EW9pDbGkLsbopyH17+vjRc13ceN4Kez2F39fjhw3LsVpjICJiF2YupAsrP4iesa2G6bCEYTyRJp7OFI1/wKQFspwF5FXABuDlwOuB15lfK4ZS6neVUl8p2NajlHqN+fyoUupi83G+UuoTlVyPZvE40D/BcDTF1RtaqPN7iCazRY+LpTJFL9qVwro4dxQIiNslNNX6GJiYuharCt3iws56XAKvOG8yifHc9hDjiUxeurLRidfRgqM2fxiSxcmhWF7Nh+VGa68P8IZtK8kpuGfX7N1YSin29Y5zbnsIEeHGc9t49NAgg5EUb37Z6inHrzUF4+ljwwS8Llrrqi9byMKqCK9cED1ttyqZjnpHP6xoMltSQFY31uB2CVtXLk5vq9kyo4AopU4opU4AcYzMJ+uh0Sw41l3s1RuaqQuUjoFEk1lGY2nS2dyirGvy4jz1TtBq0lfISCxFoyNd9KJVDbzwN6/kwlWTAegt7caFwRlIN4Rn8jzbAikIpP/Zd5/nwz/abb+2Augr64NsXBHi3PYQ9+6evYAMRJIMRVOcZ07qu/G8Ffb3ef2WqYkorSE/Aa+LSDLD6saaRauCngu2BRJcuDTeoM9tV6KPx8u0QOx+WBliqamzQCzWtdTy4t+80q5qrzZmFBAReYOIHAKOAQ8Dx4GfVXhdmrOUJ48Msb6llpUNQUJ+T8kYiOUyGF6kHlG948UtEDCCswNF0mxHoil79oNF4d3pljYjE8sZB3GOW4XJi15hKm/3aJxDjhTgvrEEQa/bTiG96bw2njs5OqXOxGLniRE71djJfjOAfq4pblesb6Y15Oe3rlhTtK2MlWEG1eu+srCsowUPoqezZiv3dFnvbf2OxuNpYqnSLixY/GmOs6EcF9bfA1cCB5VS64EbgacquirNWcsLp0a5bF0jYPzjJNK5KVZGLqfs9g+L5cbqG4vjdokdNHfSGvIzWMSFNRJL0ziDr72+xkt7ODBFQJxB3sYaHy7J/15zOcVgJEXvWMIuROsdT+T157p+SyvZnLL7bjkZjCR561ef5KM/npp7ss/svXRehyFuPo+LRz5wA39246aS34clINWagWVRmRiIh2xOkcrmGC8zBuJ0YcVSWWqqWCSmoxwBSSulhgCXiLiUUg9h9MfSaBaUZCbLUNSYnwFQZ/4jRgsC6YlMFivmXG6PqPnSO5agLTS1KR4Yd7UDkWReIDyTzTGeSOe5sEphBdItxmL5d7FWnMUpIKPxNFkzrddKn+0bS+QF+beZ2V4PHZhalX7fnj4yOcW9u3s5WtBSfn/fBB31gTw3WtDnzmtnUohVab+qjJbjS4nlDlzIGIiVLRdNZokkM2VnYYERRI+lMtTMUDdSrZQjIKMiUgc8AnxbRL4AlD/cWaMpk9PjxgXSaj5nme6FcRBnYH2xutQWXpydtNT5SWVyTDiEzsjxZ0YLBIxA+uGBCJlsjkzWeJ/CC1xLnZ+BiUmxdAbtSwmIx+3i2k0tPHxwYEoNyT27elnVGMTndvHlXx3J22cF0GfDGjN1t9otkK0dYer8HlYuYMNBy/1kZbyFy7BALCvFjoEUmQWyHChHQG4BYsD7gfuAI1Q4C0tzdmL9A64wp7dZAlKYymu5bGAxXVgJOkqkUraEzCwpx0XdbqRYO7MFsnVlmFQmx96ecbs6udDFUhiodz4/OhAlm1P0mS4sJzdsWcHARNJuCQ7Gz/npY0P8xqWreNvla7jr+W47JTqVyXFkIMK5HbPL+rlkTSN+j6tqs4Usrtvcyq6/eaXdqHIhCJoC0m/eAJWTheVxu6jze2wXVu08G18uFeUIyG3ABqVURin1daXUF02XlkazoFj/gNb8aMuFVSgg+RZI5V1YSil6xxJ0hEtbIJDvTrOq0MtxYV23qRWXGG3SC6vQJz/DV1RAXGJYIIORJNmcmpIldp1ZNf8rhxvrvj195BS87qIO/uC6cwD4j0eOAkajwXRW2RlY5XLx6gb2//3NtvuxmpnOFTcXrAyqfjNTLxwsTwzCAQ/jiTTR5PRB9GqmnO90DfBVEVkP7MBwZT2qlCo6p0OjmSvOOgZwWCCJ0hbIwCJYIOPxDPF0tqQLy+qS6nQrjUTLF5DGWh/b1zXxi32neblZI1LMhTVoxllExP6s81fWc3QwOlmnUiByrSE/F62q56EDA7zv5UYQ/Kcv9rKlLcQmMwPsTZd28p1nTtJRH+BgvxEPOW8Oc8yrOX23ktTYFojxOyh3zkg46GU0liKZyc279f5SUU4dyN8opV4ObAUeBT4A7Kz0wjRnH/0TCbxuseMGlp94otACMXPuRRbHAukdN9w7pQb1FGtnMmq7sMq7mLxyaxv7esfZ22O0Tp/iwgr5SaRz9vc+EEnic7u4eHU9Rwci9JouqGIid/3mVp4/OcLh0xP0jSV49sQwr72ow97/nus3IgL/9LP9/PC5LtrDAda31Ja1bo3DhTVhCUiZFkjQaxeQnrEWiIj8NfBrQB3wPPCXGEKi0Swop8eTrAhNpqHW+Y2L6BQLxBSUjnBgxhjIscEoP9vTa2dtver8djaumDrgB+D+vX1cvKphykW4t8AyKqRYmu3wLFxYADee18Y/3LOPH+402r8VWiBW/cLARJI6v4eBiSQtdT42tNYxkciwt8eIcRSrU3ntRSv5yiNHuelzj9DZEEQp8gRkvVmsls4aPyS/x1WVY4SrFevi3zdWfgzEOs7KgFuuQfRypPJNQAZjCuHDwJNKqeoZ0Kw5YyjMIiqVxmvdha9uqrFHqZbikz/bx/17J9uRH+yf4Au3XTLluFPDMf7wmztpqPHyT7deyKsvnLzA9pVoY2JhpNnmB7lHYil8blfZd5brW2rZuKKO504abeELi9FaQpNWzvqWWgYjKVpCfttSeOLIID63K6/3lsWW9hCPfOAGfvhcFz/Y2cXVG5qnTMnze9ws01KEJafGa/zgTs/aApkcVbxcLZByXFiXYnTHfQZ4BbBbRB6r9MI0Zx/9E4m8+dE1XjciU11YVgzEaI+eX3/hJJtTPHFkiDe/bBUH/uFmLlvXaFsThVhZSD63i/d8+zk+/MNddp1F71gClzDtRLjWkH9KDKSx1juruMBNjh5ZxYLoMJnpNTiRpLXOzzkthhC82DVGu6OIsJD2+gDvvWEjD/3l9fzvH1xZ9po0MxO0LZDZxUCcv+MzNgYiIhcAbwfeCbwV6AZ+WeF1ac5CLBeWhcsl1Pk8U1xYVhbWmqYa0lnFeLx4v6zd3WNMJDJct7kVv8dNe33QDnQWYm3/xrsv512/tp47nz1ltyfvHY3TGvLjncatU9jOxKhCn12/pVdsNXpOBb1u/J78O9LWgjjLQCRJS52fTrOWw8jAKm4haSqLZT0MRpIEvK6y58E7XV1nchrvJ4Ew8EXgPKXUDUqpj1V2WZqzjUgyQySZoa0gi6guMLUfViyVQQQ7ZbRUJpazMSNAe9hP/3iiqMVi3T2uaqzhL165Ga9bePSwMQK5bzxRtImik9a6/HYmowWNFMth2+pGmmt9RdtsGFMNYSCSIptTDEdTtJqV8WvNduqlXGyaymJVoudU+dYH5Lspg2ewC+t1GOIxpJSa+4xMjWYaLAugvT7fTVTn9xStA6n1eezsp1LV6I8dGmRrR9huX9EWDpBI54paLL1jCer8Hur8Hmr9Hi5Z02j3kOodS7ByhotzS2gyzRaMJo/lZmBZuF3C269Yw5XnNE/Z53G7aKwxakFGYoaIWG6tc1qNOEipLDFNZXG5hIDXuJSWU4Vu4XRh1S7TIHo5LqzXAy9gVKEjIttE5O5KL0xzdmEJSFso/0JdW2SsbTydIehzT1aAF0nljaey7DwxwjWbWuxtlnXTX2TIUv94fvzl2o0t7O0ZZyiSnLaNiUVLnY+ko51JYUv2cvnzV27h82/dVnRfa50RZ7HcWK3mz2q9GQfRFsjSYcUwZmWBOMTGCsQvN8pxYX0cY974KIBZQLh+vh8sIm8Wkb0ikhOR7QX7PiIih0XkgIi8qsT560XkafO474rIwjX41yw6Vh+sFQUurFCglAXitocDFc7JAHjm+DCpbI5f2zhVQIrNLzfcVJOfbQnP/Xv7iSQzM16crQD74ESSntE4Q9EUqxe4KrslZFggg2ZPLNsCMTOxdAxk6bDcWLNpE+88drmm8ZbbjXesYNtCDJTag5Ei/Ihzo4hsxWifcj5wM/DvIlLsp/sp4PNKqY3ACPDuBViTZonoGy9ea1HnnxpEN+YneOy4QLFW6o8fNtJardbwAO2WBVIkkN4/lsiLv1y0qoFwwMP3d54y1zW9e8jZzsSaAvjqC9qnPWe2WNXoAxFj/ZZoXXlOM5vb6rjIMahKs7hYgfRyU3ihMAvrzBWQvSLyW4BbRDaJyL8CT8z3g5VS+5RSB4rsugW4UymVVEodAw5jWEA2YuQqvhz4gbnp68Ab57smzdLRP56g1ueeMjynZAzE7zbqL2p8DBYZKvXYoUEuXduQlx65wnRRnS4QnFxOcXoiaQsMGPGIqze08LxZlzGTBeKsRv/prh4u7Kxn3QJXc7fU+RmcSE1aIKaArGmu4efv/3UdA1lCLAGYTQzEskBEIOA5cwXk/8OwBpLAd4Ax4E8ruKZO4JTjdZe5zUkzMKqUykxzDAAicruI7BCRHQMDAwu+WM3c+cHOLnsOxunx5JQMLDCzsEpYIGBdVPMFYShidJ+9xuG+Agh43TTUeKe4sAajSTJF0mCd8ZP2Eo0ULSwB2XlihBe7xnido9J7oWip8xNPZzk+FMXncRHSlX9VQ9AWkNnHQGq8089aqWbKycKKKaU+qpS6TCm1Hfgm8G/lvLmIPCAie4o8bpnvwstFKXWHUmq7Ump7a+vUec6apSGdzfGhH+7iiw8eAgwX1orw1EK9kN9DJJXJS72NprJ21kpznY+hAgvkW0+dBIz2IIW0hQJTXFj9Y9YcknyRuLZIAL4UTbVGO5Pv7TDufV5bEQExYh77+yZorfOftc0Lq5HJIHr5ol7n9+ASCC7TGhCYppWJiFwE/DOwEvgx8CUM4bgC+Gw5b66UumkOa+oGVjterzK3ORkCGkTEY1ohxY7RVDE9o3GyOcWTR4fI5RT94wm2r22cclxdwINSGDMTzDvuWDLfAnmxa9Q+fjSW4j8fPcqrzm8r2pK8rX6qgNjxlwKRWNtcy+qmIPFUbsbiMGc7k0vXNFSkrbkV89jfO87Gttl3y9VUDtsCmUUQXUQIB73LNoUXprdA/gP4X+A3gEGMVN4jwEal1OcruKa7gdtExG+2kN+E0UbFRhm3ow8Bv2lueifwkwquSbPAWD2shqMp9vdNlHZhWQ0VHXGQaMrIwgLTAnGk8d7xyFEiqQzvf8Xmop/bFvLbc0cs+sZKd7L93avX84aLV5b1PVkWwuvLPH62WG6yaCprV6ZrqgNrJO1sLBAwXF7BZTrOFqYXEL9S6n+UUgeUUv8CRJVSH1RKFe8FMUtE5FYR6QKuAu4RkfsBlFJ7ge8BL2HUnrxXKZU1z7lXRKz/zg8Bfy4ihzFiIv+1EOvSLA4nhyebIN67u5dUNjclhRcmGyo6a0FiqQw1/kkLJJLMkEhnGYwk+e/Hj/P6i1ZybnvxgUht4QAD5vAli77xBG6X2BdoJ+++Zj0fe/3Wsr6n1pAfEXjNhQvvvrLef/K5zlqvJuwsLP/sikfDQY9tWS9Hplt5QEQuASxHa9L5Win13Hw+WCl1F3BXiX2fAD5RZPtrHM+PUpCdpVk+nByO4fO4WFkf4K7nDe9jsUB1qGCsbSqTI51VtgVi3fW/cGqUrz9xnGQmy5/dtKnk57bVB8jmFEORpC1YfWNGY0L3PAOZrzq/nQ2tdTPGS+aKs9OutkCqCyuOMRsXFsBrL1zJMo2fA9MLSC/wOcfrPsdrhZFGq9HMiZNDMVY3BrnynGa+/bQR9G4rEkS3x9qaFojVidcZAwG47Y6ncAn88fUbOae1+LwPMFxYYAXtJ+tCFqII7x1Xrp33e0yH1+2iscbLSCxtp/BqqoO51IEAvOf6DZVYzqJR8rtVSt2wmAvRnF2cGI6xtrmWqze0OARk6kXc6lJqNVS0ZoFYgceLVzdw1TnNXLa+ibdetprOhulrISyhcMZB+sYTbJxGdKqJljo/I7G0tkCqjJo5BNHPBJav802zbFFKcWo4xhXrm7hqw2TjwGLzNkIFMRBrGqHTAvnO7eXPt7DbmTgysfrHElNqRqqV1pCfQ6cj2gKpMq7a0MxrL+ywLdyzBT23UrPojMTSRJIZ1jTV0FTrY2tHmMYaL4Ei2Sh1BTGQWIEFMlta6vy4BE6bAhJNZpgo0ka+WrFcdsUC/pql4/yV9Xzp7ZeedaOAtQWiWXRODEUBYyAUwB9ct55jA9Gix1oZKlYMJGrGQIJz7F7qdgmtIb9djd5Xoo18tWIJx3TTETWaxWLG/0Kz79TbgXOUUn8nImuAdqXUMzOcqtEUxUrhtQYh3XrJqpLH+jwu/B7XpAWSnJ8FAka2V7/Z/sQSkuVigdx8QTvJzGQdjEazlJRzG/fvQA4j6+rvgAngh8BlFVyX5gzmpFlEuLqpvGrtUMBjz9mIFmRhzYUV4YC9BktAZup1VS1cvr6Jy9c3LfUyNBqgvBjIFUqp9wIJAKXUCKCrmDRz5uRwjLawv2jMoxh1fg/RBYqBgCEWPWNGK5VSbeQ1Gs3MlDUPxJzHoQBEpBXDItFo5sSJ4Zgd/ygHZ0feaHL+FsjVG5qZSGT478eP0T+eIBTwzOv9NJqzlXIE5IsYFeMrROQTwGPAP1Z0VZozmlPDMdY0lT8ro84/6cKyLJD5DOC5+YJ2bjx3Bf/88wPsPDGybNxXGk21UU47928DHwT+CaM6/Y1Kqe9XemGaM5NEOkvfeGJ2Fojfm5eF5fO48M4jXVJE+MStF+J1udjbM67dVxrNHCn5XygiTdYDOI0xTOp/gX5zm0Yza7pG4ig1mYFVDnV+NxNmJXosuTAZSO31Af76decByycDS6OpNqZz/O7EiHsIsAZj7rgADcBJYH3FV6c54zg5bNR7lJuBBUbNQ/94klxOEXVMI5wvb9m+mhNDMa7dpAeNaTRzYbpeWOsBROQ/gLuUUvear1+Nnj+umQW5nOIvvv8izY5usrOxQNa11JLK5OgdTxgWyAIN4BERPnjzuQvyXhrN2Ug5t3JXKqX+wHqhlPqZiHy6gmvSnGH0TyTslu1gBMCdYjIT65uNgPvxweiCWiAajWZ+lPOf2CMifw18y3z9dqCnckvSnGn0jBq1Fp/+jYsYiCSp83tmNc97XYshIMcGo+ZoW12FrdFUA+WksrwNaMVI5b0LWGFumzMi8mYR2SsiORHZ7tj+ChHZKSK7za9FZ46IyMdFpFtEXjAfryl2nKY66Bk1RsZetLqe996wkXdevW5W57eHA/g9Lk4MGQKiLRCNpjqY8T9RKTUM/KmIhIyXKrIAn7sHeBPw1YLtg8DrlVI9InIBcD/QWeI9Pq+U+ucFWIumwvSaM8dXzjCroxQul7C2uYZjgzFjnK3uA6XRVAXlNFO8EPgG0GS+HgTeqZTaM9cPVUrtM9+rcPvzjpd7gaCI+JVSSTTLlp7RBHV+D+HA3IftrGuu5ehglGhSWyAaTbVQjgvrq8CfK6XWKqXWAn8B3FHZZQHwG8Bz04jH+0Rkl4h8TUQaS72JiNwuIjtEZMfAwEBlVqqZlp7ROCsb5ldrsb6llpNDMSLJtO5Eq9FUCeUISK1S6iHrhVLqV8CMfShE5AER2VPkcUsZ554PfAr4wxKHfBnYAGzDqI7/bKn3UkrdoZTarpTa3tqq8/2Xgp6xOB31c3NfWaxrqSWVzZFI56jxawtEo6kGyvlPPCoi/z/wTfP1O4CjM52klLppLgsSkVUYwfrfUUodKfHe/Y7j/wP46Vw+S7M49I4muLCzYV7vsa558p5FWyAaTXVQjgXyLowsrB+ZjxZz24IjIg3APcCHlVKPT3Nch+PlrRhBeU0VkkhnGYqm6FwAF5aFtkA0muqgnGaKI0qpP1FKXYoxROpj5kyQOSMit4pIF3AVcI+I3G/ueh+wEfiYI0V3hXnOfzpSfj9tpvruAm4A3j+f9WgqR685sGm+LqwVIT8Br/Hnqi0QjaY6KCcL63+BPwKywLNAWES+oJT6zFw/VCll1ZQUbv8H4B9KnPP7jue/PdfP1iwuVg3IXFN4LVwuYV1zLfv7JnQWlkZTJZTjwtqqlBrH6H/1M4wmivoCrimLSQGZf8dbKw6iK9E1muqgHAHxiogXQ0DuVkqlMacTajQAf/G9F/nM/fuL7rPamCzEzA2rpYm2QDSa6qDcOpDjGKm7j4jIWmC8kovSLB/GE2l+/EI3dz5zilxu6n1F71icljo/fs/8rYZzTAEJBbSAaDTVQDmtTL6IMdbW4oSI3FC5JWmWE08cHiSbUwxFU+zpGeOiVfnput2j8XlnYFm87uIOMjnFphV1C/J+Go1mfpQUEBF5h1LqWyLy5yUO+VyF1qRZRjx8cJCg100ik+XhAwNTBKR3LMHG1oW54Nf4PPzWFWsW5L00Gs38mc6FZSXeh0o8NGc5SikeOTjAtZtauLCznocPDkzZb7QxmV8GlkajqU6mm0j4VfPr3y7ecjTLiSMDUbpH47zn+g30jyf40kOHGYulqa8xmiaOxzPEUtkFycDSaDTVx4xBdBE5R0T+T0QGROS0iPxERM5ZjMVpqhvL4vj1za38+uZWcgoeOzxo7+9eoBoQjUZTnZSThfW/wPeADmAl8H3gO5VclGZ58MjBAc5pqWV1Uw3bVjcQDnh4+OBpe781B6RjAVJ4NRpN9VGOgNQopb6plMqYj28B+opwljORSPPU0SGu22x0OPa4XVyzqYVHDk5aIF0jhoB0agtEozkjKUdAfiYiHxaRdSKyVkQ+CNwrIk0i0lTpBWqqkx/s7CKZyXHrJZMDI7evbaJvPEH/uFE8uLt7jOZaH60h/1ItU6PRVJByKrLeYn4tnM1xG0ZFuo6HnGXkcoqvP3GcS9c0cPHqybTdi1bVA7Cra4xXbA3wwqlRtq1umDJ5UqPRnBmUU0i4fjEWolk+PHxwgONDMf78lVvytm9dGcYlsLtrlMvXN3H4dIQ3blu5RKvUaDSVpqQLy3RVWc/fXLDvHyu5KE11899PHKct7OfVF7Tnba/xedjcFmJX9xi7ukYB2La65LRhjUazzJkuBnKb4/lHCvbdXIG1aJYBfWMJHjk4wG9dvhave+qfz4Wd9ezuGuP5k6OIwEWr65dglRqNZjGYTkCkxPNir2eFiLxZRPaKSM4xJAozUB93DJP6Sonzm0TkFyJyyPyqb3MXieNDUQAuXVt8RO1Fq+oZiqa4d3cvG1vrCAe8i7k8jUaziEwnIKrE82KvZ8se4E3AI0X2HVFKbTMff1Ti/A8DDyqlNgEPmq81i8BkbUfx1NwLzV5Y+/sm2LZ6fnPQNRpNdTNdEP1iERnHsDaC5nPM1/OqA1FK7QPmk51zC3C9+fzrwK+AD81nTZrSpLM5UpkctX6PPd+jVHuSc9tDeFxCJqfYtkYLiEZzJlPSAlFKuZVSYaVUSCnlMZ9bryvpl1gvIs+LyMMicm2JY9qUUr3m8z6grYLrOatJpLO89atPctsdTwGGBVIf9JYc6hTwutnSbvTavEQH0DWaM5qKTeYRkQeA9iK7PqqU+kmJ03qBNUqpIRF5GfBjETnfHKlbFKWUEpGSLjURuR24HWDNGt0KfDZkc4o/u/MFnjs5is/jIpdT9I4mZmxNcumaRk4Ox9jcpud2aDRnMhUTEKXUTXM4Jwkkzec7ReQIsBnYUXBov4h0KKV6RaQDOE0JlFJ3AHcAbN++XY/inQW/eKmP+/b2cUFnmD3d4wxGkvSMJWZsjviXr9rCO69eh6dIlpZGozlzqKr/cBFpFRG3+fwcYBNwtMihdwPvNJ+/Eyhl0WjmwcnhGAB/cK3RbODUSJzesfiMFkh90MtGPTVQoznjWRIBEZFbRaQLuAq4R0TuN3ddB+wSkReAHwB/pJQaNs/5T0fK7yeBV4jIIeAm87VmgRmKpPB5XJzbHgbg8OkJRmNp3Z5do9EAFXRhTYdS6i7griLbfwj8sMQ5v+94PgTcWLEFagAYiCRprfOzqtEQjGeOjQC6PbtGozGoKheWproYjKRoqfNR6/fQVOvj2ePDQOkaEI1Gc3ahBURTksGJJC11Riv2VY1BOyaiR9RqNBrQAqKZhqFokuY6H4DtxgJo1y4sjUaDFhBNCXI5xVAk5bBAagBoqfPh97iXcmkajaZK0AKisTk+GCWRzgIwFk+Tyak8Fxbo+IdGo5lEC4gGMFqWvPoLj/LNJ08AMBhJAtASKhQQ7b7SaDQGWkA0AHSPxomnsxwz27UPWAJSa8VADBeWrgHRaDQWWkA0APSMGm3a+8aMbrtDkRQwaYGsbqwh5PfYjRI1Go1mSQoJNdVH90i+gNguLDMGEvS5efRDNxDSA6I0Go2JFhANYLiwAPrGJwXE7RIagpOC0VDjW5K1aTSa6kS7sM5CDvRNMGRaGHu6xxiNpWwLZDiaIpHOMjiRornWh8s1r+nFGo3mDEZbIGch7/qfZ1nXUsMn33QRt/7747z9irV0mRYIwOnxpFlE6F/CVWo0mmpHC8hZyEAkSfdonD+583nSWcULp0YZMNuWDEaS9I0nGDD7YGk0Gk0ptAvrDCWXUzx8cACl8mdoJdJZUpkcAM+fHCXodfNS7zh94wlettaYYd47FmdwwujEq9FoNKXQAnKG8ot9/bzza8/wzLHhvO3j8TQA57TWUh/08pev2kIqkyObU7xsjRxhKQAAE6hJREFUrTHDvG8swWAkaafwajQaTTG0gJyhPHlkCICukXje9vGEISB/euMmnv6rG7lhS6u9b0t7mFqfmyMDEZKZnHZhaTSaaVmqiYRvFpG9IpJzTBlERN4uIi84HjkR2Vbk/I+LSLfjuNcs7ndQ/Tx11BAQKy3XYsy0QOqDXgJeN+uaawn5jVBYZ0OQtvoAv9w/ABjFgxqNRlOKpbJA9gBvAh5xblRKfVsptU0ptQ34beCYUuqFEu/xeetYpdS9FV7vsmIkmmJ/3wRgxDMAIskMD+7rZzyeAQwBAXC5hPM7jZG1nQ1BOuoDDEaSrKwPcON5bUuweo1Gs1xYEgFRSu1TSh2Y4bC3AXcuxnrONCzrw+sW+saMeo+v/OoI7/76Do4MRAAIOwoEb9iygvNXhgn63LSFjWaJt193Dj6P9nBqNJrSVPMV4q3Ad6bZ/z4R2SUiXxORxlIHicjtIrJDRHYMDAws/CqrjGePD/ORu3bTUufn8vVN9I3HUUpx755eAA6fNgXE0ZLkD399A/f8ybUAXLK6gXXNNbz1sjWLv3iNRrOsqJiAiMgDIrKnyOOWMs69AogppfaUOOTLwAZgG9ALfLbUeyml7lBKbVdKbW9tbS112BnDFx44RMDj5ofvuYo1TTX0jSU5dDrC0QGjy+7RQeNrOFi8BOi3r1rHQ395PUGfHhql0Wimp2KFhEqpm+Zx+m1MY30opfqt5yLyH8BP5/FZZxSnJxJsW93A2uZa2sNBBiNJ7n6hx95/bDBKwOuadqqgiG5fotFoZqbqXFgi4gLewjTxDxHpcLy8FSMor8Fow27NMW+vN+o47nz2JFvajDbsAxPJPPeVRqPRzJWlSuO9VUS6gKuAe0Tkfsfu64BTSqmjBef8pyPl99MisltEdgE3AO9flIVXOZlsjuFYyu5h1W6Onx2MpHjz9lX43Mavuz6oBUSj0cyfJemFpZS6C7irxL5fAVcW2f77jue/XbHFLWNGYmmUglbTAnGOn735gnb+67Fj9I4l8jKwNBqNZq5UnQtLM3eGokbKrmWBWCm5F62qZ1Vjje3a0haIRqNZCHQ33jOIwQljDG2zOcc8HPBw2bpG3rJ9tbndb2/XaDSa+aKvJGcQhRaIiPD9P7ra3m9ZINqFpdFoFgItIGcASim+/fRJjps1HqXasFvzzbULS6PRLARaQM4Ajg/F+Osf78El4HFJySJBq7uuTuPVaDQLgQ6iL1OGIkm+/KsjZLI5njZ7X+WU4aYqVQhoxUC0BaLRaBYCbYEsU+56vptP3befc9tDeUOjLJEoxmQMRP/aNRrN/NEWyDJlX6/Rrv2e3b08fWzYzqyaborgBZ31XLKmgQtXNSzKGjUazZmNFpBlyr7ecQB+uquH7tE477x6HQAttaWnCLbU+bnrj3+NzobgYixRo9Gc4WhfxjIknc1x+HSEK89pom8sgdft4i3bV9M9GueGLSuWenkajeYsQQvIMuTIQIRUNsfbLl/DLds67e2fe8uU6b8ajUZTMbQLaxmy88QIAFs7wku8Eo1GczajBaQKGIok+cd795HK5Kbse/TQAO/51k6+8eRxAJ44PMjf/t9LbGkLsb6ldnEXqtFoNA60gFQBv9x/mjseOcqurtEp+7722DF+tqePz9x/AKUU//3EcZpqfNx5+5V43PrXp9Folg59BaoCTk8YPazu39vHP5tCYXGw35hhPpHI0DuW4KmjQ1y/pZXGabKtNBqNZjHQArJIDEdT/PWPdxNPZafs6x9PAPCfjx3j3x46zNHBKI8dGuQz9++nezTOtZtaAPjxC91MJDJctaF5Udeu0Wg0xVgyARGRz4jIfhHZJSJ3iUiDY99HROSwiBwQkVeVOH+9iDxtHvddEamKW/J/ffAQT5mtRZx86aHDfOupk/zwuS4AvvjgITsYbgmIZXg8eWSIrz1+jC89dASA11+0EoD/fvw4AFedowVEo9EsPUtpgfwCuEApdRFwEPgIgIhsBW4DzgduBv5dRNxFzv8U8Hml1EZgBHj3oqx6GlKZHJ/9xUFuu+OpKfuyOUMdJhIZUpkcn/vFQX78fDcA/ePJvGOfPDrEnu4x+/VVG5oJet0MTCTZvraRFeEAGo1Gs9QsmYAopX6ulMqYL58CVpnPbwHuVEollVLHgMPA5c5zxegW+HLgB+amrwNvrPyqp6d3LG4/T6TzXVVWXGMwkmQwYgjGwET+V4Ban5t7dvXacRGAzoYgcfP9PvTqcyuzeI1Go5kl1RIDeRfwM/N5J3DKsa/L3OakGRh1CFCxYwAQkdtFZIeI7BgYGFiQxX7pocM88FL/lO2nhicF5Mu/OsJHfrSbnGl59JluqhNDUVswDp2e4E/vfJ7u0Thul9FB9/evPcd+j9+5ai0feNUWXC7h337rEv7kxk1ctq5pQb4HjUajmS8VrUQXkQeA9iK7PqqU+ol5zEeBDPDtSqxBKXUHcAfA9u3b1QyHz0g6m+Mz9x8A4PgnX5u379RIzH7+hQcPAXCgb5x3XLmWvjFDQI4NRm3r4shAlCMDxhCot162GgHee8NG+9wPvGoLIXN2x+vMOIhGo9FUCxUVEKXUTdPtF5HfBV4H3Kgmc1e7gdWOw1aZ25wMAQ0i4jGtkGLHLCj/8sBBnjg8xG9uX1XymK6RGB6X8LbL1/DNp04A8NzJUZ47OUqr2SX31HA8z9Vlce3GFl59YQcAX3/X5TxzbMgWD41Go6lGlqwXlojcDHwQ+HWlVMyx627gf0Xkc8BKYBPwjPNcpZQSkYeA3wTuBN4J/KSS6/2vx44xkciwr2/c3vaTF7rZ1zvBh27egohwajhOR0OA91y/gXg6y5XnNPMvDxykayTOwMT/a+/Og6QozziOf3+cwoIssBAN1wICKqgIiCgqGg0qJqDxKJEjRLSiUVMpApVUWRUTjRpLY6VMFA9CiEeMihdFFOONQREwLHdAWIiiRkHwAC+EJ3+878K4LDuzDTs9uzyfqq6d6WP6eWam951+u/vpL+nT4UCWvvMJD857e7fXz7zJ05Ce7RjSs11tpuOcc3stzWMgfwJaAs9KKpN0J4CZLQMeBpYDs4ArzGw7gKSnJFX05fwCmCBpNeGYyJ9rK9BNW7/i0y/C4ZaKvwCTpi/mzpfXcM8r5UDowurUujnfLm7GLecfxXn9OzL9suN3zj9yYGc6FDfbWYodoHFDcdGxnenXpXVthe+cc7UitT2QePrtnqZdD1xfxfhhGY/LqXR2Vm1ZuzFcDd6tpIjyjVtp0bQRW74Mp+N2Kyniplkr6XXQgSx79xPGDOryjWUPanUAV55yCGs2bOHkXu356LNtO4+hAPRo35IbzjkiH2k459w+VShnYRW0igPd1wzvzdjjujB13DE7pz16+fF0adOcS+9dwFdf7+CEQ0p2W37i6b2YPLo/HYqbccGATjRuKJrEOlYHt/JrOpxzdZPfDyQH5Ru20rihGNy9LUN6tmPb9h38aHApYwZ1oXVREyaP7s/Zt8+hUQMxsGv1p9m2a9mUa0f0oW1RE+av28SlGaftOudcXeINSA5K2zbnB0d33Fn9tnHDBlzz/d47p/c6qCV3jOpH+catFDXN/paOHNgZgKG9qzrD2Tnn6gZlVn6t7wYMGGALFixIOwznnKtTJL1hZgMqj/djIM455xLxBsQ551wi3oA455xLxBsQ55xziXgD4pxzLhFvQJxzziXiDYhzzrlEvAFxzjmXyH51IaGkDcB/a7BICbCxlsLJF8+hMHgOhcFzSKaLme12j4n9qgGpKUkLqrr6si7xHAqD51AYPId9y7uwnHPOJeINiHPOuUS8Aane3WkHsA94DoXBcygMnsM+5MdAnHPOJeJ7IM455xLxBsQ551wi3oAAks6QtFLSakm/rGJ6U0kPxemvSyrNf5TVyyGHCZKWS1os6XlJXdKIszrZcsiY71xJJqkgTmXMlEsOki6In8UySX/Ld4zZ5PBd6izpRUkL4/dpWBpx7omkqZI+kLR0D9Ml6baY32JJ/fIdYzY55DAqxr5E0quSjsp3jACY2X49AA2BNUA3oAmwCDi80jw/Ae6Mjy8EHko77gQ5nAI0j48vr4s5xPlaArOBucCAtONO8Dn0ABYCrePz9mnHnSCHu4HL4+PDgXVpx10pvpOAfsDSPUwfBjwNCBgEvJ52zAlyOD7jO3RmWjn4HggMBFabWbmZfQX8HRhRaZ4RwF/j4+nAqZKUxxizyZqDmb1oZp/Fp3OBjnmOMZtcPgeA64CbgC/yGVyOcsnhUuB2M9sMYGYf5DnGbHLJwYAD4+NWwLt5jC8rM5sNbKpmlhHAvRbMBYolHZyf6HKTLQcze7XiO0SK27M3INABeDvj+fo4rsp5zOxr4GOgbV6iy00uOWQaT/gFVkiy5hC7GjqZ2T/yGVgN5PI59AR6Spojaa6kM/IWXW5yyeHXwGhJ64GngKvyE9o+U9PtpdCltj03SmOlLj2SRgMDgCFpx1ITkhoAtwLjUg5lbzUidGOdTPjVOFvSEWb2UapR1cxIYJqZ/V7SccB9kvqY2Y60A9vfSDqF0ICckMb6fQ8E3gE6ZTzvGMdVOY+kRoTd9g/zEl1ucskBSacBVwPDzezLPMWWq2w5tAT6AC9JWkfou55RYAfSc/kc1gMzzGybma0FVhEalEKRSw7jgYcBzOw14ABCgb+6IqftpdBJOhKYAowws1T+H3kDAvOBHpK6SmpCOEg+o9I8M4AfxsfnAS9YPHpVILLmIOlo4C5C41Fo/e6QJQcz+9jMSsys1MxKCf2+w81sQTrhVimX79IThL0PJJUQurTK8xlkFrnk8BZwKoCkwwgNyIa8Rrl3ZgBj49lYg4CPzey9tIOqCUmdgceAMWa2KrVA0j7boBAGwlkZqwhnn1wdx11L+AcFYQN5BFgNzAO6pR1zghyeA94HyuIwI+2Ya5pDpXlfosDOwsrxcxChK245sAS4MO2YE+RwODCHcIZWGTA07Zgrxf8g8B6wjbDHNx64DLgs4zO4Pea3pEC/R9lymAJsztieF6QRp5cycc45l4h3YTnnnEvEGxDnnHOJeAPinHMuEW9AnHPOJeINiHPO1VPZijJWMX+NCn36WViuXpN0I/BPwsWfhxEuGhtMKBTYFVgZZ/0tcDFwkdXCVeGSiuNr37GXr3Mh0N3Mrt83ke1xPdOAmWY2vTbX42qXpJOALYTaX32yzNuDcIHod8xss6T2luWaMd8DcfXdsYSLDocAs83sCjPrS7jWYY2Z9Y3DdDMbVhuNR1RMqOq8t84EZu2D13H7AauiKKOk7pJmSXpD0iuSDo2Talzo0xsQVy9JulnSYuAY4DXgEmCypF9Vs8w6SSWSSiX9R9I0SaskPSDptFgA8U1JA+P8RbGLYF68N8aIOL53HFcW79nQA/gd0D2OuznON0nS/DjPb+K4inU/IGmFpOmSmsdpAvoC/65m3eMkPSnppRjrNRn5TZC0NA4/yxg/NsawSNJ9GW/JSfFeE+WSzovzHixpdsxjqaQT9/rDcvl2N3CVmfUHJgIVe8U1L/SZ9hWXPvhQWwOh8fgj0BiYU2laKZXutQCsI9R0KgW+Bo4g/Mh6A5hKuIJ5BPBEnP8GYHR8XEy4ersornNUHN8EaFZ5fcDQuCErrmMm4R4QpYRy6YPjfFOBifFxP0JXRHXrHke4grltXO9SQvHM/oSrrouAFsAy4Gigd1y2JL5Wm/h3GqH6QgPCleer4/ifs+vq9IZAy7Q/Zx+ybgc7v3vxs/+cXVewlwEr4rSZwONxe+lKqFhcXN1rezVeV5/1I5TbOBRYUcNl15rZEgBJy4DnzcwkLSFskBAageGSJsbnBwCdCXs8V0vqCDxmZm9q99vHDI3Dwvi8BaGo4lvA22Y2J46/H/gpcAtwBrvKdu9p3QDPWiyuJ+kxQqVWAx43s60Z40+M4x8xs40AZpbZ3fGEhQq7yyV9K46bD0yV1DhOL8vyPrrC0gD4yEI3bmXrCTem2gaslVRR6HP+nl7MGxBX70jqS/gF3RHYCDQPo1UGHGdmn+fwMpnVindkPN/Bru1GwLlmtpJvWiHpdeAs4ClJP2b3gokCbjSzuyrFXkr4p56p4vlQ4Nzq1i3p2GqWr6nM90AQ+tTjgdmzgGmSbjWzexO+vsszM/tE0lpJ55vZI7Fb9EgzW0Qo9DkS+EuuhT79GIird8ysLP7CWkXofnkBON3CwfJcGo9cPQNcFTfCiorHSOoGlJvZbcCTwJHAp4SS9JnLXiypRVymg6T2cVpnhftsAFwE/EtSK6CR7SrbXeW6o+9KaiOpGXA2ofDhK8DZkppLKgLOieNeAM6X1Da+TpvqEpbUBXjfzO4hFPQruPuJu10kPUjYI+4lab2k8cAoYLykRYSuzIo7Tj4DfChpOfAiMMmylIn3PRBXL0lqB2w2sx2SDjWz5bWwmuuAPwCLFW54tRb4HnABMEbSNuB/wA1mtikenFwKPG1mkxRKob8W24AtwGhgO+HU4iskTSVU7Z1MOGvsuRzWDaFi9KOEPbD7LZa8Vzg1d16cZ4qZLYzjrwdelrSd0KU2rpqcTwYmxdy2AGNr9I65vDKzkXuYtNsBcgsHQibEISd+HYhzBSR2Yc20SufsS5pC+Kc/N8vy4wjlya+srRidq+B7IM7VAWZ2SdoxOFeZ74E455xLxA+iO+ecS8QbEOecc4l4A+Kccy4Rb0Ccc84l4g2Ic865RP4PLAFvYqT39aAAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.plot(episode_num, mean_reward)\n",
        "plt.xlabel('#Episode')\n",
        "plt.ylabel('Episode Reward')\n",
        "print('max episode no. is {} ; max epoch/timestep is {}'.format(episode_num[-1],epochs[-1] ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "fE22UJG-kefH",
        "outputId": "4b70c52a-ffd4-423d-f332-315403376948"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max episode no. is 1570 ; max epoch/timestep is 1255366\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}